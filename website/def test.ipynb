{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    wd = \"C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver\"\n",
    "    driver = webdriver.Chrome(wd)\n",
    "    driver.get(url)\n",
    "    pages = 0\n",
    "    try:\n",
    "        while True:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"#cbox_module > div > div.u_cbox_paginate > a\"))).click()\n",
    "            time.sleep(1.5)\n",
    "            print(pages, end=\" \")\n",
    "            pages+=1\n",
    "    except exceptions.ElementNotVisibleException as e: # 페이지 끝\n",
    "        pass\n",
    "    \n",
    "    except Exception as e: # 다른 예외 발생시 확인\n",
    "        print(e)\n",
    "    html = driver.page_source\n",
    "    dom = BeautifulSoup(html, \"lxml\")\n",
    "    # 댓글이 들어있는 페이지 전체 크롤링\n",
    "    comments_raw = dom.find_all(\"span\", {\"class\" : \"u_cbox_contents\"})\n",
    "    # 댓글의 text만 뽑는다.\n",
    "    comments = [comment.text for comment in comments_raw]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p=comment('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㅋㅋㅋㅋㅋ이 4명 케미 왜이리 웃김 ㅋㅋㅋㅋ 수다만 떨었는데도 빵빵터짐 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ',\n",
       " '의미부여없이 웃기기로는 놀면뭐하니에서 오늘이 젤웃겼음ㅋㅋ지석진 조세호냅두고선 문닫고도망치는거 최고의 킬링포인트ㅋㅋㅋㅋ',\n",
       " '유느 행복해하는거 넘 보기좋네요ㅎㅎㅎ',\n",
       " '오프닝으로 뽕뽑넼ㅋㅋㅋㅋㅋ개웃겨',\n",
       " '유느가 기뻐하면 좋아요😆😆😆😆 토크만으로도 웃김',\n",
       " '유재석 저렇게 즐거워 죽을 거 같은 얼굴로 미친듯이 웃는 거 근 5년 만에 처음 보는 듯. 나머지 셋 보고 웃는 건데 오히려 그 셋은 덤덤하고 혼자 웃겨서 뒤로 넘어가려는 게 너무 웃겨ㅋㅋㅋㅋㅋㅋㅋㅋ',\n",
       " '유재석 방송 아닌 진짜웃는것같아 보는사람도 조았어요. 그리고 실제로도 책마니 읽는다하네요..괜히 1인자가 아님',\n",
       " '런닝맨에서 수년을봤는데도 광수나오니까 뭔가신선함',\n",
       " '지석진 땀닦은 휴지보고 이광수가 살점떨어진줄 알았다고 한겈ㅋㅋㅋㅋㅋㅋㅋ진짜웃겨',\n",
       " '소중한 우리 산슬이가 타고있어요 ㅋㅋㅋㅋㅋㅋㅋ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(comments):\n",
    "    token_sent=[]\n",
    "    for comment in comments:\n",
    "        temp = okt.morphs(comment, stem=True)\n",
    "        token_sent.append(temp)\n",
    "        max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # 상위 35,000개의 단어만 보존\n",
    "    tokenizer.fit_on_texts(token_sent) \n",
    "    token_sent = tokenizer.texts_to_sequences(token_sent)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index)+1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(token_sent, maxlen=max_len)\n",
    "    model = load_model('model11.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    for i in range(len(X_data)):\n",
    "        if(predict[i] == 0):\n",
    "            a=0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articeBody'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\t\\t\\t'놀면 뭐하니' 유재석, 절친♥︎들과 남산→서점→방 탈출..포상휴가 '만끽'[종합]\\n\\t\\t\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = soup.select_one('h2.end_tit')\n",
    "    text_r = text.get_text()\n",
    "    return text_r\n",
    "title = get_title('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 놀면 뭐하니 유재석 절친 들과 남산 서점 방 탈출..포상휴가 만끽 종합 '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "\n",
    "def keyword_sentence(text):#키워드 문장 추출\n",
    "    data = text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=10, num_keysents=10)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 놀면 뭐하니 방송화면 캡처 김보라 기자 유재석 지석진 이광수 조세호가 포상 휴가를 즐겼다',\n",
       " ' 조세호도 전문적으로 웃음을 드린다 고 소개했는데 유재석이 전문적인 거 치곤 조금 아쉽다 고 하자 조세호는 근근이 밥 벌어 먹고 산다 고 받아쳤다',\n",
       " ' 이광수의 의견에 따라 유재석은 남산 돈가스 집까지 걸어 가자고 했다',\n",
       " ' 조세호와 이광수도 평상시에 오는 건 좋은데 휴가에 여길 오냐 고 했다',\n",
       " ' 아내와 아이와 행복을 위해 달려나가고 있다 고 했고 이광수는 저는 개그맨은 아니지만 주말 예능을 하고 있다 고 말해 웃음을 안겼다',\n",
       " ' 세 사람은 유재석과 단둘이 대 로 떠나는 여행인 줄 알고 왔는데 알고 보니 인이 떠나는 여행이 됐다',\n",
       " ' 지석진은 이게 이 프로그램의 위엄이다',\n",
       " ' 이게 뭐냐 런닝맨 이냐 라고 비하했다',\n",
       " ' 걷는 걸 좋아한다 며 여행하면서 차 타는 것도 좋지만 걸으며 보는 걸 좋아한다 고 말했다',\n",
       " ' 이들은 아침 시에 연 남산 돈가스 집에서 아침식사를 했다']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = keyword_sentence(clean)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 놀면 뭐하니 방송화면 캡처 김보라 기자 유재석 지석진 이광수 조세호가 포상 휴가를 즐겼다',\n",
       " ' 조세호도 전문적으로 웃음을 드린다 고 소개했는데 유재석이 전문적인 거 치곤 조금 아쉽다 고 하자 조세호는 근근이 밥 벌어 먹고 산다 고 받아쳤다',\n",
       " ' 이광수의 의견에 따라 유재석은 남산 돈가스 집까지 걸어 가자고 했다',\n",
       " ' 조세호와 이광수도 평상시에 오는 건 좋은데 휴가에 여길 오냐 고 했다',\n",
       " ' 아내와 아이와 행복을 위해 달려나가고 있다 고 했고 이광수는 저는 개그맨은 아니지만 주말 예능을 하고 있다 고 말해 웃음을 안겼다',\n",
       " ' 세 사람은 유재석과 단둘이 대 로 떠나는 여행인 줄 알고 왔는데 알고 보니 인이 떠나는 여행이 됐다',\n",
       " ' 지석진은 이게 이 프로그램의 위엄이다',\n",
       " ' 이게 뭐냐 런닝맨 이냐 라고 비하했다',\n",
       " ' 걷는 걸 좋아한다 며 여행하면서 차 타는 것도 좋지만 걸으며 보는 걸 좋아한다 고 말했다',\n",
       " ' 이들은 아침 시에 연 남산 돈가스 집에서 아침식사를 했다']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
