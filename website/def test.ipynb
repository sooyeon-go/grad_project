{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='realArtcContents'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£\\.]+\", \" \", str(text))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'html.parser', from_encoding='utf-8')\n",
    "    text = soup.find('h3',class_='articleSubecjt')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\") \n",
    "    comments = [] \n",
    "    for i in soup.select('dl#cmt_item > dd'): \n",
    "        comments.append(i.get_text().strip(\"\\n\\t \"))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = comment('https://news.nate.com/view/20200310n00423')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ë§ˆì´ë°ì¼ë¦¬ ì´ìŠ¹ê¸¸ ê¸°ì ë§Œí™”ê°€ ê¸°ì•ˆ ê°€ ì½”ë¡œë‚˜ ì™€ ì‹¸ìš°ê³  ìˆëŠ” ëŒ€êµ¬ì— ë§ˆìŠ¤í¬ë¥¼ ê¸°ë¶€í–ˆë‹¤. ê¸°ì•ˆ ëŠ” ì¼ ìì‹ ì˜ ì¸ìŠ¤íƒ€ê·¸ë¨ì— ëŒ€êµ¬ì‹œì²­ì— ë§ˆìŠ¤í¬ ë§Œì¥ ê¸°ë¶€í–ˆìŠµë‹ˆë‹¤. ë‚´ì¼ ë„ì°©í•œë‹¤ëŠ”ë° ë„ì›€ ëìœ¼ë©´ ì¢‹ê² ë„¤ìš”. ê°ì‚¬í•©ë‹ˆë‹¤. ê¾¸ë²… ì´ë€ ê¸€ì„ ë‚¨ê²¼ë‹¤. ì¼ ê¸°ì¤€ìœ¼ë¡œ êµ­ë‚´ ì½”ë¡œë‚˜ í™•ì§„ìëŠ” ëª…ê¹Œì§€ ëŠ˜ì—ˆë‹¤. í™•ì§„ì ìˆ˜ê°€ ëŠ˜ì–´ê°€ë©´ì„œ ìŠ¤íƒ€ë“¤ì€ ë°”ì´ëŸ¬ìŠ¤ì™€ ì‹¸ìš°ê³  ìˆëŠ” ì˜ë£Œì§„ í™˜ìë“¤ì„ ìœ„í•´ ê¸°ë¶€ ë“±ìœ¼ë¡œ ì‘ì›ì˜ ëœ»ì„ ì „í•˜ê³  ìˆë‹¤. ì‚¬ì§„ ë§ˆì´ë°ì¼ë¦¬ ì‚¬ì§„ ê¸°ì•ˆ ì¸ìŠ¤íƒ€ê·¸ë¨ ì´ìŠ¹ê¸¸ ê¸°ì . . ì„±ì¸ ì±„ë„ì—ì„œë‚˜ ê¹€ë¯¼ê²½ ì„±í¬ë¡± í™©ë‹¹ ì—ë²„ê¸€ë¡œìš° ì•„ìƒ¤ íŠœë¸Œí†± ì…ê³  ê³µí•­ ë“±ì¥ í—‰ ê¹€ìŠ¹í˜„ ì•„ë‚´ì™€ ë‹¤íˆ¼ ê³ ë°± ìŠ¤ì¼€ì¤„ ê³µìœ  ê·¸ì € ê°íƒ„ë§Œ ì¡°ì„¸íœ˜ êµ¬ë¦¿ë¹› ëª¸ë§¤ ê³µê°œ ì–‘ì¤€ì¼ ì¶œì—°ë£Œ ë¹„ì‹¸ê²Œ ë¶€ë¥¸ë‹¤ ì˜í˜¹ ì§„ì‹¤ì€ ë§ˆì´ë°ì¼ë¦¬ . . . . ë¬´ë‹¨ì „ì¬ ì¬ë°°í¬ ê¸ˆì§€ . . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "t = get_text('https://news.nate.com/view/20200305n25115')\n",
    "cl = clean_text(t)\n",
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    wd = \"C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver\"\n",
    "    driver = webdriver.Chrome(wd)\n",
    "    driver.get(url)\n",
    "    pages = 0\n",
    "    try:\n",
    "        while True:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"#cbox_module > div > div.u_cbox_paginate > a\"))).click()\n",
    "            time.sleep(1.5)\n",
    "            print(pages, end=\" \")\n",
    "            pages+=1\n",
    "    except exceptions.ElementNotVisibleException as e: # í˜ì´ì§€ ë\n",
    "        pass\n",
    "    \n",
    "    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ë°œìƒì‹œ í™•ì¸\n",
    "        print(e)\n",
    "    html = driver.page_source\n",
    "    dom = BeautifulSoup(html, \"lxml\")\n",
    "    # ëŒ“ê¸€ì´ ë“¤ì–´ìˆëŠ” í˜ì´ì§€ ì „ì²´ í¬ë¡¤ë§\n",
    "    comments_raw = dom.find_all(\"span\", {\"class\" : \"u_cbox_contents\"})\n",
    "    # ëŒ“ê¸€ì˜ textë§Œ ë½‘ëŠ”ë‹¤.\n",
    "    comments = [comment.text for comment in comments_raw]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p=comment('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ì˜ë¯¸ë¶€ì—¬ì—†ì´ ì›ƒê¸°ê¸°ë¡œëŠ” ë†€ë©´ë­í•˜ë‹ˆì—ì„œ ì˜¤ëŠ˜ì´ ì ¤ì›ƒê²¼ìŒã…‹ã…‹ì§€ì„ì§„ ì¡°ì„¸í˜¸ëƒ…ë‘ê³ ì„  ë¬¸ë‹«ê³ ë„ë§ì¹˜ëŠ”ê±° ìµœê³ ì˜ í‚¬ë§í¬ì¸íŠ¸ã…‹ã…‹ã…‹ã…‹',\n",
       " 'ìœ ëŠ í–‰ë³µí•´í•˜ëŠ”ê±° ë„˜ ë³´ê¸°ì¢‹ë„¤ìš”ã…ã…ã…',\n",
       " 'ì˜¤í”„ë‹ìœ¼ë¡œ ë½•ë½‘ë„¼ã…‹ã…‹ã…‹ã…‹ã…‹ê°œì›ƒê²¨',\n",
       " 'ìœ ëŠê°€ ê¸°ë»í•˜ë©´ ì¢‹ì•„ìš”ğŸ˜†ğŸ˜†ğŸ˜†ğŸ˜† í† í¬ë§Œìœ¼ë¡œë„ ì›ƒê¹€',\n",
       " 'ìœ ì¬ì„ ì €ë ‡ê²Œ ì¦ê±°ì›Œ ì£½ì„ ê±° ê°™ì€ ì–¼êµ´ë¡œ ë¯¸ì¹œë“¯ì´ ì›ƒëŠ” ê±° ê·¼ 5ë…„ ë§Œì— ì²˜ìŒ ë³´ëŠ” ë“¯. ë‚˜ë¨¸ì§€ ì…‹ ë³´ê³  ì›ƒëŠ” ê±´ë° ì˜¤íˆë ¤ ê·¸ ì…‹ì€ ë¤ë¤í•˜ê³  í˜¼ì ì›ƒê²¨ì„œ ë’¤ë¡œ ë„˜ì–´ê°€ë ¤ëŠ” ê²Œ ë„ˆë¬´ ì›ƒê²¨ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹',\n",
       " 'ìœ ì¬ì„ ë°©ì†¡ ì•„ë‹Œ ì§„ì§œì›ƒëŠ”ê²ƒê°™ì•„ ë³´ëŠ”ì‚¬ëŒë„ ì¡°ì•˜ì–´ìš”. ê·¸ë¦¬ê³  ì‹¤ì œë¡œë„ ì±…ë§ˆë‹ˆ ì½ëŠ”ë‹¤í•˜ë„¤ìš”..ê´œíˆ 1ì¸ìê°€ ì•„ë‹˜',\n",
       " 'ëŸ°ë‹ë§¨ì—ì„œ ìˆ˜ë…„ì„ë´¤ëŠ”ë°ë„ ê´‘ìˆ˜ë‚˜ì˜¤ë‹ˆê¹Œ ë­”ê°€ì‹ ì„ í•¨',\n",
       " 'ì†Œì¤‘í•œ ìš°ë¦¬ ì‚°ìŠ¬ì´ê°€ íƒ€ê³ ìˆì–´ìš” ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹',\n",
       " 'ì°¨íƒ€ê³  ì´ë™í• ë•Œ ì†Œì¤‘í•œ ìœ ì‚°ìŠ¬ì´ íƒ€ê³  ìˆìŠµë‹ˆë‹¤.ã…‹ã…‹íƒœí˜¸PDì˜ ìœ ëŠë‹˜ì— ëŒ€í•œ ì• ì • ë³´ì´ì§€ ì•ŠëŠ” ê³³ì—ë„ í˜ëŸ¬ ë„˜ì³ ë‹´ì£¼ë„ ë¬´ë„ ë³¸ë°© ì‚¬ìˆ˜~',\n",
       " 'ìœ ëŠ ì •ë§ ì¢‹ì•„í•˜ëŠ”ê²Œ ëˆˆì— ë³´ì´ë“œë¼ã…ã…ã…ã…']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(comments):\n",
    "    token_sent=[]\n",
    "    for comment in comments:\n",
    "        temp = okt.morphs(comment, stem=True)\n",
    "        token_sent.append(temp)\n",
    "        max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # ìƒìœ„ 35,000ê°œì˜ ë‹¨ì–´ë§Œ ë³´ì¡´\n",
    "    tokenizer.fit_on_texts(token_sent) \n",
    "    token_sent = tokenizer.texts_to_sequences(token_sent)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index)+1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(token_sent, maxlen=max_len)\n",
    "    model = load_model('model11.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    for i in range(len(X_data)):\n",
    "        if(predict[i] == 0):\n",
    "            a=0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articeBody'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\t\\t\\t'ë†€ë©´ ë­í•˜ë‹ˆ' ìœ ì¬ì„, ì ˆì¹œâ™¥ï¸ë“¤ê³¼ ë‚¨ì‚°â†’ì„œì â†’ë°© íƒˆì¶œ..í¬ìƒíœ´ê°€ 'ë§Œë½'[ì¢…í•©]\\n\\t\\t\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = soup.select_one('h2.end_tit')\n",
    "    text_r = text.get_text()\n",
    "    return text_r\n",
    "title = get_title('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ë†€ë©´ ë­í•˜ë‹ˆ ìœ ì¬ì„ ì ˆì¹œ ë“¤ê³¼ ë‚¨ì‚° ì„œì  ë°© íƒˆì¶œ..í¬ìƒíœ´ê°€ ë§Œë½ ì¢…í•© '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "def keyword_sentence(url):#í‚¤ì›Œë“œ ë¬¸ì¥ ì¶”ì¶œ\n",
    "    m_text = get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "    r_text = clean_text(m_text)\n",
    "    data = r_text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=11, num_keysents=10)\n",
    "    keyword = list(keywords.keys())\n",
    "    return keyword[0], sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ë†€ë©´ ë­í•˜ë‹ˆ ë°©ì†¡í™”ë©´ ìº¡ì²˜ ê¹€ë³´ë¼ ê¸°ì ìœ ì¬ì„ ì§€ì„ì§„ ì´ê´‘ìˆ˜ ì¡°ì„¸í˜¸ê°€ í¬ìƒ íœ´ê°€ë¥¼ ì¦ê²¼ë‹¤',\n",
       " ' ì¡°ì„¸í˜¸ë„ ì „ë¬¸ì ìœ¼ë¡œ ì›ƒìŒì„ ë“œë¦°ë‹¤ ê³  ì†Œê°œí–ˆëŠ”ë° ìœ ì¬ì„ì´ ì „ë¬¸ì ì¸ ê±° ì¹˜ê³¤ ì¡°ê¸ˆ ì•„ì‰½ë‹¤ ê³  í•˜ì ì¡°ì„¸í˜¸ëŠ” ê·¼ê·¼ì´ ë°¥ ë²Œì–´ ë¨¹ê³  ì‚°ë‹¤ ê³  ë°›ì•„ì³¤ë‹¤',\n",
       " ' ì´ê´‘ìˆ˜ì˜ ì˜ê²¬ì— ë”°ë¼ ìœ ì¬ì„ì€ ë‚¨ì‚° ëˆê°€ìŠ¤ ì§‘ê¹Œì§€ ê±¸ì–´ ê°€ìê³  í–ˆë‹¤',\n",
       " ' ì´ì— ì¡°ì„¸í˜¸ëŠ” ìš°ë¦¬ê°€ ë§Œë§Œí•œ ê±°ë‹¤ ë¼ê³  ë§ì•„ì³ ì›ƒìŒì„ ì•ˆê²¼ë‹¤',\n",
       " ' ì¡°ì„¸í˜¸ì™€ ì´ê´‘ìˆ˜ë„ í‰ìƒì‹œì— ì˜¤ëŠ” ê±´ ì¢‹ì€ë° íœ´ê°€ì— ì—¬ê¸¸ ì˜¤ëƒ ê³  í–ˆë‹¤',\n",
       " ' ì•„ë‚´ì™€ ì•„ì´ì™€ í–‰ë³µì„ ìœ„í•´ ë‹¬ë ¤ë‚˜ê°€ê³  ìˆë‹¤ ê³  í–ˆê³  ì´ê´‘ìˆ˜ëŠ” ì €ëŠ” ê°œê·¸ë§¨ì€ ì•„ë‹ˆì§€ë§Œ ì£¼ë§ ì˜ˆëŠ¥ì„ í•˜ê³  ìˆë‹¤ ê³  ë§í•´ ì›ƒìŒì„ ì•ˆê²¼ë‹¤',\n",
       " ' ì„¸ ì‚¬ëŒì€ ìœ ì¬ì„ê³¼ ë‹¨ë‘˜ì´ ëŒ€ ë¡œ ë– ë‚˜ëŠ” ì—¬í–‰ì¸ ì¤„ ì•Œê³  ì™”ëŠ”ë° ì•Œê³  ë³´ë‹ˆ ì¸ì´ ë– ë‚˜ëŠ” ì—¬í–‰ì´ ëë‹¤',\n",
       " ' ìš°ë¦¬ ë„·ì´ ì–´ë”” ê°€ëŠ” ê±° ì²˜ìŒì´ë‹¤ ë¼ê³  ê¸°ë»í•˜ëŠ” ìœ ì¬ì„',\n",
       " ' ì§€ì„ì§„ì€ ì´ê²Œ ì´ í”„ë¡œê·¸ë¨ì˜ ìœ„ì—„ì´ë‹¤',\n",
       " ' ê±·ëŠ” ê±¸ ì¢‹ì•„í•œë‹¤ ë©° ì—¬í–‰í•˜ë©´ì„œ ì°¨ íƒ€ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ê±¸ìœ¼ë©° ë³´ëŠ” ê±¸ ì¢‹ì•„í•œë‹¤ ê³  ë§í–ˆë‹¤']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword, sents = keyword_sentence(clean)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ìº¡ì²˜'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
    "    global keyword\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    init_word = current_word \n",
    "    sentence = ''\n",
    "    for _ in range(n): # në²ˆ ë°˜ë³µ\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # í˜„ì¬ ë‹¨ì–´ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # ë°ì´í„°ì— ëŒ€í•œ íŒ¨ë”©\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ yë¥¼ ì˜ˆì¸¡í•˜ê³  y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
    "        for k, index in t.word_index.items(): \n",
    "            if index == result: # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´\n",
    "                break # í•´ë‹¹ ë‹¨ì–´ê°€ ì˜ˆì¸¡ ë‹¨ì–´ì´ë¯€ë¡œ break\n",
    "        current_word = current_word + ' '  + k # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
    "        sentence = sentence + ' ' + k # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
    "    # forë¬¸ì´ë¯€ë¡œ ì´ í–‰ë™ì„ ë‹¤ì‹œ ë°˜ë³µ\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def clean_sentence(word):\n",
    "    global keyword\n",
    "    max_words = 35000\n",
    "    clean_sen = []\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'k' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-8a187819ae4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclean_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-ba012d1d4d07>\u001b[0m in \u001b[0;36mclean_sentence\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclean_sen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mclean_sen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-00ef959e2e49>\u001b[0m in \u001b[0;36msentence_generation\u001b[1;34m(current_word, n)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mbreak\u001b[0m \u001b[1;31m# í•´ë‹¹ ë‹¨ì–´ê°€ ì˜ˆì¸¡ ë‹¨ì–´ì´ë¯€ë¡œ break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcurrent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_word\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;31m# í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;31m# ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# forë¬¸ì´ë¯€ë¡œ ì´ í–‰ë™ì„ ë‹¤ì‹œ ë°˜ë³µ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'k' referenced before assignment"
     ]
    }
   ],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
    "    global word\n",
    "    init_word = current_word # ì²˜ìŒ ë“¤ì–´ì˜¨ ë‹¨ì–´ë„ ë§ˆì§€ë§‰ì— ê°™ì´ ì¶œë ¥í•˜ê¸°ìœ„í•´ ì €ì¥\n",
    "    sentence = ''\n",
    "    for _ in range(n): # në²ˆ ë°˜ë³µ\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # í˜„ì¬ ë‹¨ì–´ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # ë°ì´í„°ì— ëŒ€í•œ íŒ¨ë”©\n",
    "        result = model.predict_classes(encoded, verbose=0)# ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ yë¥¼ ì˜ˆì¸¡í•˜ê³  y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´\n",
    "                break # í•´ë‹¹ ë‹¨ì–´ê°€ ì˜ˆì¸¡ ë‹¨ì–´ì´ë¯€ë¡œ break\n",
    "        current_word = current_word + ' '  + word # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
    "        sentence = sentence + ' ' + word # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
    "    # forë¬¸ì´ë¯€ë¡œ ì´ í–‰ë™ì„ ë‹¤ì‹œ ë°˜ë³µ\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(word):\n",
    "    clean_sen = []\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(model, t, word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
