{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    wd = \"C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver\"\n",
    "    driver = webdriver.Chrome(wd)\n",
    "    driver.get(url)\n",
    "    pages = 0\n",
    "    try:\n",
    "        while True:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"#cbox_module > div > div.u_cbox_paginate > a\"))).click()\n",
    "            time.sleep(1.5)\n",
    "            print(pages, end=\" \")\n",
    "            pages+=1\n",
    "    except exceptions.ElementNotVisibleException as e: # í˜ì´ì§€ ë\n",
    "        pass\n",
    "    \n",
    "    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ë°œìƒì‹œ í™•ì¸\n",
    "        print(e)\n",
    "    html = driver.page_source\n",
    "    dom = BeautifulSoup(html, \"lxml\")\n",
    "    # ëŒ“ê¸€ì´ ë“¤ì–´ìˆëŠ” í˜ì´ì§€ ì „ì²´ í¬ë¡¤ë§\n",
    "    comments_raw = dom.find_all(\"span\", {\"class\" : \"u_cbox_contents\"})\n",
    "    # ëŒ“ê¸€ì˜ textë§Œ ë½‘ëŠ”ë‹¤.\n",
    "    comments = [comment.text for comment in comments_raw]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p=comment('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ã…‹ã…‹ã…‹ã…‹ã…‹ì´ 4ëª… ì¼€ë¯¸ ì™œì´ë¦¬ ì›ƒê¹€ ã…‹ã…‹ã…‹ã…‹ ìˆ˜ë‹¤ë§Œ ë–¨ì—ˆëŠ”ë°ë„ ë¹µë¹µí„°ì§ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹',\n",
       " 'ì˜ë¯¸ë¶€ì—¬ì—†ì´ ì›ƒê¸°ê¸°ë¡œëŠ” ë†€ë©´ë­í•˜ë‹ˆì—ì„œ ì˜¤ëŠ˜ì´ ì ¤ì›ƒê²¼ìŒã…‹ã…‹ì§€ì„ì§„ ì¡°ì„¸í˜¸ëƒ…ë‘ê³ ì„  ë¬¸ë‹«ê³ ë„ë§ì¹˜ëŠ”ê±° ìµœê³ ì˜ í‚¬ë§í¬ì¸íŠ¸ã…‹ã…‹ã…‹ã…‹',\n",
       " 'ìœ ëŠ í–‰ë³µí•´í•˜ëŠ”ê±° ë„˜ ë³´ê¸°ì¢‹ë„¤ìš”ã…ã…ã…',\n",
       " 'ì˜¤í”„ë‹ìœ¼ë¡œ ë½•ë½‘ë„¼ã…‹ã…‹ã…‹ã…‹ã…‹ê°œì›ƒê²¨',\n",
       " 'ìœ ëŠê°€ ê¸°ë»í•˜ë©´ ì¢‹ì•„ìš”ğŸ˜†ğŸ˜†ğŸ˜†ğŸ˜† í† í¬ë§Œìœ¼ë¡œë„ ì›ƒê¹€',\n",
       " 'ìœ ì¬ì„ ì €ë ‡ê²Œ ì¦ê±°ì›Œ ì£½ì„ ê±° ê°™ì€ ì–¼êµ´ë¡œ ë¯¸ì¹œë“¯ì´ ì›ƒëŠ” ê±° ê·¼ 5ë…„ ë§Œì— ì²˜ìŒ ë³´ëŠ” ë“¯. ë‚˜ë¨¸ì§€ ì…‹ ë³´ê³  ì›ƒëŠ” ê±´ë° ì˜¤íˆë ¤ ê·¸ ì…‹ì€ ë¤ë¤í•˜ê³  í˜¼ì ì›ƒê²¨ì„œ ë’¤ë¡œ ë„˜ì–´ê°€ë ¤ëŠ” ê²Œ ë„ˆë¬´ ì›ƒê²¨ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹',\n",
       " 'ìœ ì¬ì„ ë°©ì†¡ ì•„ë‹Œ ì§„ì§œì›ƒëŠ”ê²ƒê°™ì•„ ë³´ëŠ”ì‚¬ëŒë„ ì¡°ì•˜ì–´ìš”. ê·¸ë¦¬ê³  ì‹¤ì œë¡œë„ ì±…ë§ˆë‹ˆ ì½ëŠ”ë‹¤í•˜ë„¤ìš”..ê´œíˆ 1ì¸ìê°€ ì•„ë‹˜',\n",
       " 'ëŸ°ë‹ë§¨ì—ì„œ ìˆ˜ë…„ì„ë´¤ëŠ”ë°ë„ ê´‘ìˆ˜ë‚˜ì˜¤ë‹ˆê¹Œ ë­”ê°€ì‹ ì„ í•¨',\n",
       " 'ì§€ì„ì§„ ë•€ë‹¦ì€ íœ´ì§€ë³´ê³  ì´ê´‘ìˆ˜ê°€ ì‚´ì ë–¨ì–´ì§„ì¤„ ì•Œì•˜ë‹¤ê³  í•œê²ˆã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ì§„ì§œì›ƒê²¨',\n",
       " 'ì†Œì¤‘í•œ ìš°ë¦¬ ì‚°ìŠ¬ì´ê°€ íƒ€ê³ ìˆì–´ìš” ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(comments):\n",
    "    token_sent=[]\n",
    "    for comment in comments:\n",
    "        temp = okt.morphs(comment, stem=True)\n",
    "        token_sent.append(temp)\n",
    "        max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # ìƒìœ„ 35,000ê°œì˜ ë‹¨ì–´ë§Œ ë³´ì¡´\n",
    "    tokenizer.fit_on_texts(token_sent) \n",
    "    token_sent = tokenizer.texts_to_sequences(token_sent)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index)+1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(token_sent, maxlen=max_len)\n",
    "    model = load_model('model11.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    for i in range(len(X_data)):\n",
    "        if(predict[i] == 0):\n",
    "            a=0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articeBody'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\t\\t\\t'ë†€ë©´ ë­í•˜ë‹ˆ' ìœ ì¬ì„, ì ˆì¹œâ™¥ï¸ë“¤ê³¼ ë‚¨ì‚°â†’ì„œì â†’ë°© íƒˆì¶œ..í¬ìƒíœ´ê°€ 'ë§Œë½'[ì¢…í•©]\\n\\t\\t\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = soup.select_one('h2.end_tit')\n",
    "    text_r = text.get_text()\n",
    "    return text_r\n",
    "title = get_title('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ë†€ë©´ ë­í•˜ë‹ˆ ìœ ì¬ì„ ì ˆì¹œ ë“¤ê³¼ ë‚¨ì‚° ì„œì  ë°© íƒˆì¶œ..í¬ìƒíœ´ê°€ ë§Œë½ ì¢…í•© '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "\n",
    "def keyword_sentence(text):#í‚¤ì›Œë“œ ë¬¸ì¥ ì¶”ì¶œ\n",
    "    data = text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=10, num_keysents=10)\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ë†€ë©´ ë­í•˜ë‹ˆ ë°©ì†¡í™”ë©´ ìº¡ì²˜ ê¹€ë³´ë¼ ê¸°ì ìœ ì¬ì„ ì§€ì„ì§„ ì´ê´‘ìˆ˜ ì¡°ì„¸í˜¸ê°€ í¬ìƒ íœ´ê°€ë¥¼ ì¦ê²¼ë‹¤',\n",
       " ' ì¡°ì„¸í˜¸ë„ ì „ë¬¸ì ìœ¼ë¡œ ì›ƒìŒì„ ë“œë¦°ë‹¤ ê³  ì†Œê°œí–ˆëŠ”ë° ìœ ì¬ì„ì´ ì „ë¬¸ì ì¸ ê±° ì¹˜ê³¤ ì¡°ê¸ˆ ì•„ì‰½ë‹¤ ê³  í•˜ì ì¡°ì„¸í˜¸ëŠ” ê·¼ê·¼ì´ ë°¥ ë²Œì–´ ë¨¹ê³  ì‚°ë‹¤ ê³  ë°›ì•„ì³¤ë‹¤',\n",
       " ' ì´ê´‘ìˆ˜ì˜ ì˜ê²¬ì— ë”°ë¼ ìœ ì¬ì„ì€ ë‚¨ì‚° ëˆê°€ìŠ¤ ì§‘ê¹Œì§€ ê±¸ì–´ ê°€ìê³  í–ˆë‹¤',\n",
       " ' ì¡°ì„¸í˜¸ì™€ ì´ê´‘ìˆ˜ë„ í‰ìƒì‹œì— ì˜¤ëŠ” ê±´ ì¢‹ì€ë° íœ´ê°€ì— ì—¬ê¸¸ ì˜¤ëƒ ê³  í–ˆë‹¤',\n",
       " ' ì•„ë‚´ì™€ ì•„ì´ì™€ í–‰ë³µì„ ìœ„í•´ ë‹¬ë ¤ë‚˜ê°€ê³  ìˆë‹¤ ê³  í–ˆê³  ì´ê´‘ìˆ˜ëŠ” ì €ëŠ” ê°œê·¸ë§¨ì€ ì•„ë‹ˆì§€ë§Œ ì£¼ë§ ì˜ˆëŠ¥ì„ í•˜ê³  ìˆë‹¤ ê³  ë§í•´ ì›ƒìŒì„ ì•ˆê²¼ë‹¤',\n",
       " ' ì„¸ ì‚¬ëŒì€ ìœ ì¬ì„ê³¼ ë‹¨ë‘˜ì´ ëŒ€ ë¡œ ë– ë‚˜ëŠ” ì—¬í–‰ì¸ ì¤„ ì•Œê³  ì™”ëŠ”ë° ì•Œê³  ë³´ë‹ˆ ì¸ì´ ë– ë‚˜ëŠ” ì—¬í–‰ì´ ëë‹¤',\n",
       " ' ì§€ì„ì§„ì€ ì´ê²Œ ì´ í”„ë¡œê·¸ë¨ì˜ ìœ„ì—„ì´ë‹¤',\n",
       " ' ì´ê²Œ ë­ëƒ ëŸ°ë‹ë§¨ ì´ëƒ ë¼ê³  ë¹„í•˜í–ˆë‹¤',\n",
       " ' ê±·ëŠ” ê±¸ ì¢‹ì•„í•œë‹¤ ë©° ì—¬í–‰í•˜ë©´ì„œ ì°¨ íƒ€ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ê±¸ìœ¼ë©° ë³´ëŠ” ê±¸ ì¢‹ì•„í•œë‹¤ ê³  ë§í–ˆë‹¤',\n",
       " ' ì´ë“¤ì€ ì•„ì¹¨ ì‹œì— ì—° ë‚¨ì‚° ëˆê°€ìŠ¤ ì§‘ì—ì„œ ì•„ì¹¨ì‹ì‚¬ë¥¼ í–ˆë‹¤']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = keyword_sentence(clean)\n",
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ë†€ë©´ ë­í•˜ë‹ˆ ë°©ì†¡í™”ë©´ ìº¡ì²˜ ê¹€ë³´ë¼ ê¸°ì ìœ ì¬ì„ ì§€ì„ì§„ ì´ê´‘ìˆ˜ ì¡°ì„¸í˜¸ê°€ í¬ìƒ íœ´ê°€ë¥¼ ì¦ê²¼ë‹¤',\n",
       " ' ì¡°ì„¸í˜¸ë„ ì „ë¬¸ì ìœ¼ë¡œ ì›ƒìŒì„ ë“œë¦°ë‹¤ ê³  ì†Œê°œí–ˆëŠ”ë° ìœ ì¬ì„ì´ ì „ë¬¸ì ì¸ ê±° ì¹˜ê³¤ ì¡°ê¸ˆ ì•„ì‰½ë‹¤ ê³  í•˜ì ì¡°ì„¸í˜¸ëŠ” ê·¼ê·¼ì´ ë°¥ ë²Œì–´ ë¨¹ê³  ì‚°ë‹¤ ê³  ë°›ì•„ì³¤ë‹¤',\n",
       " ' ì´ê´‘ìˆ˜ì˜ ì˜ê²¬ì— ë”°ë¼ ìœ ì¬ì„ì€ ë‚¨ì‚° ëˆê°€ìŠ¤ ì§‘ê¹Œì§€ ê±¸ì–´ ê°€ìê³  í–ˆë‹¤',\n",
       " ' ì¡°ì„¸í˜¸ì™€ ì´ê´‘ìˆ˜ë„ í‰ìƒì‹œì— ì˜¤ëŠ” ê±´ ì¢‹ì€ë° íœ´ê°€ì— ì—¬ê¸¸ ì˜¤ëƒ ê³  í–ˆë‹¤',\n",
       " ' ì•„ë‚´ì™€ ì•„ì´ì™€ í–‰ë³µì„ ìœ„í•´ ë‹¬ë ¤ë‚˜ê°€ê³  ìˆë‹¤ ê³  í–ˆê³  ì´ê´‘ìˆ˜ëŠ” ì €ëŠ” ê°œê·¸ë§¨ì€ ì•„ë‹ˆì§€ë§Œ ì£¼ë§ ì˜ˆëŠ¥ì„ í•˜ê³  ìˆë‹¤ ê³  ë§í•´ ì›ƒìŒì„ ì•ˆê²¼ë‹¤',\n",
       " ' ì„¸ ì‚¬ëŒì€ ìœ ì¬ì„ê³¼ ë‹¨ë‘˜ì´ ëŒ€ ë¡œ ë– ë‚˜ëŠ” ì—¬í–‰ì¸ ì¤„ ì•Œê³  ì™”ëŠ”ë° ì•Œê³  ë³´ë‹ˆ ì¸ì´ ë– ë‚˜ëŠ” ì—¬í–‰ì´ ëë‹¤',\n",
       " ' ì§€ì„ì§„ì€ ì´ê²Œ ì´ í”„ë¡œê·¸ë¨ì˜ ìœ„ì—„ì´ë‹¤',\n",
       " ' ì´ê²Œ ë­ëƒ ëŸ°ë‹ë§¨ ì´ëƒ ë¼ê³  ë¹„í•˜í–ˆë‹¤',\n",
       " ' ê±·ëŠ” ê±¸ ì¢‹ì•„í•œë‹¤ ë©° ì—¬í–‰í•˜ë©´ì„œ ì°¨ íƒ€ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ê±¸ìœ¼ë©° ë³´ëŠ” ê±¸ ì¢‹ì•„í•œë‹¤ ê³  ë§í–ˆë‹¤',\n",
       " ' ì´ë“¤ì€ ì•„ì¹¨ ì‹œì— ì—° ë‚¨ì‚° ëˆê°€ìŠ¤ ì§‘ì—ì„œ ì•„ì¹¨ì‹ì‚¬ë¥¼ í–ˆë‹¤']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
