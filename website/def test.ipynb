{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='realArtcomments'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'html.parser', from_encoding='utf-8')\n",
    "    text = soup.find_all(\"h3\", {\"class\": \"articleSubject\"})\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    soup = BeautifulSoup(urllib.request.urlopen(url).read(), \"html.parser\") \n",
    "    comments = [] \n",
    "    for i in soup.find_all(\"dd\", class_=\"usertxt\"): \n",
    "        comments.append(i.get_text().strip(\"\\n\\t \"))\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' .남까내리기 좋아하는 친구랑은 손절하기 .남은 너한테 생각보다 별 관심없으니까 하고싶은대로 하며살기 .그런 잡다한 생각하지말고 공부하기 .남은 남이니 나랑 비교하지말기 .자기를 객관적으로 판단할줄 알며 자기의 단점도 받아드리기',\n",
       " '.',\n",
       " ' 년생 명회사 부사장임. 억대연봉이고 키 공부 등 자격증 개 토익 점대 보유재산 억 섹파 명임. 한번의 실패도 해 보지 않은 사람은 한번의 도전도 해 보지 않은 사람이다. 내가 가장좋아하는 말임. 매일같이 무언가에 도전해 보셈. 도전하는 자만이 성취의 기쁨을 누릴 수 있음',\n",
       " '담부턴 띄워쓰기 좀...',\n",
       " '자존감은 자신이 완벽해서 높아지는 것도 또는 완벽함을 기대하는 것이 아니라 부족한 부분이 있음에도 있는 그대로 그런 자신에게 만족하는 겁니다.',\n",
       " '누차 댓글을 달지만 반말로 쓴 글은 그닥 읽을 가치를 못 느낌. 음슴체도 아니고...꼰대스럽겠지만 그래도 최소한의 예의는 필요하다고 봄.',\n",
       " '음 .. 나도 진짜 자존감낮거든 근데 ㄹㅇ 운동꾸준히 해봐 기본적으로 자신감은 생기는거같아 얼굴이랑 몸매도 더 이뻐지고 균형있어지고 옷태도 달라지니까 난 그런데서 왠지 여유가 생기더라고 땀흘리면서 운동하면 마음가짐도 달라지는게 느껴저',\n",
       " '능력있는 남자 만나면 자존감 올라감',\n",
       " '사랑해 수고해써 어제 수고했다는 인사가 늦었다 ㅜ 어제보다 더 나은 오늘이 되길 바랄게 니가 세상에서 최고야 제일 그니까 하고 싶은 거 다 해',\n",
       " '나도 자존감 조카 낮았는데 항상 남들이 비교하는거 안들으려고 노력하고 진짜 나랑 가까이 있는 사람있잖아 뭐 부모님이라던가 년지기 절친 등.. 진짜 내가 부족한게 뭔지 물어봐서 그걸 없애려고 했던것같아',\n",
       " '방석을 좀 여러 개 깔아 봐.',\n",
       " '상처받을 때마다 나한테 절대적인 사랑을 주는 가족을 생각하기. 친구는 시간이 지나면서 변하지만 가족은 평생 내 편. 니가 날 싫어해 웅 상관없어 집에가면 날 절대적으로 사랑해주는 가족이 있어 저는 그래서 누구 욕할때 친구한테 잘 안 합니다 엄마한테 합니다 ㅋㅋㅋ 내가 당당하면 사람들이 절대 무시 못합니다 눈치보지 마세요. 당당은 어디서 오냐. 날 절대적으로 사랑해주는 사람들의 사랑에서 옵니다.',\n",
       " '저도 똑같은 성격이에요 글 읽으면서 제 얘기인가 싶을 정도로 비슷한데요 자존감은 쉽게 떨어지지만 회복하는게 정말 어려워요 저도 아직 노력중이지만 자기 자신에 대해 정확히 알았으면 좋겠어요 늘 맞추면서 살아와서 자신이 무엇을 좋아하고 싫어하는지 관심있는게 뭔지 아는게 가장 중요해요 사연자 분이 좋은분이셔서 분명히 좋은친구들이 있을거에요 자신과 잘 맞고 나라는 사람 자체를 좋아하는 분과 지내세요 그런 분들은 놓치면 정말 후회합니다 꼭 잡으세요 제가 평소에 듣던 조언들 중 도움된것만 말씀드려요 화이팅 하세요 할수있어요',\n",
       " '나는 밥을 참 잘 먹는다. 신발을 가지런히 벗는다.처럼 정말 사소한 것부터 다 칭찬해주기. 남이 아니라 어제의 나와 비교하며 나는 어제보다 공부를 분 더 했다.처럼 자신을 뿌듯이 여기고 칭찬해주기. 친구에게 넌 예민해 라고 쉽게 부정적인 말로 상처줄 수 없듯이 자신에게도 난 예민해 가 아닌 난 섬세하고 감수성이 풍부해라고 긍정적인 말로 바꿔해주기. 내가 사랑하는 사람이 좋아하는것 해주고 싶듯이 내가 좋아하는것 내가 원하는것 내가 먹고싶고 가고싶고 하고싶은것등 내가 행복한 것에 관심갖고 중요시 여기며 그것들을 해줌으로써 나를 행복하게 해주기. 내가 사랑하는 사람 애완견 에겐 이쁜말 별 사소한것도 다 칭찬 좋아하는 것 다 해주듯이 나를 사랑해주고 존중해주세요..',\n",
       " '일단 도움이 되었다니 정말다행입니다. 성격은 환경에 따라 달라지기도 하니 넘 걱정 안했으면 해요. 실은 저도 굉장히 비슷한 성격입니다. 성격이었다고 하는게 맞겠네요. 완벽주의 성향에 매우 예민한... 실은 내가 모두를 따시키고 있는데 아무도 모름 ㅎㅎ 상대방이 단점이 어느 순간 보이고 주지도 않고 받지도 않겠다는 약간은 개인주의적인 성향도 있죠. 말그대로 자존감 부족에서 오는 것들이 맞더라구요. 종이를 반으로 나누어 나의 장점 나의 단점을 각각 썼을때도 아마 단점을 많이 쓰시게 될겁니다. 쉽게 말해 나를 사랑하는게 먼저입니다. 나에게 잘해주는것 좋은것도 먹게 해주고 운동도 해주며 나를 건강하게 하는것 부터.. 깊게 파고들면 나의 존재가 얼마나 귀한 것인지에 대한 것. 한번뿐인 삶에 나는 어떤 존재로 살아가는지에 대한 성찰이 정말 필요합니다. 혼자 생각하기엔 생각이 많은 타입이라 밤새 생각해도 잘 모르실거에요. 주변에 나를 인간적으로 존중하고 전적으로 지지하는 누군가가 있다면 더더욱 빨리 좋아지실 거라고 봅니다. 쉽게 접근해서 친한척 하는 사람들 분별은 할 수 있을거라 믿고요. 뭔가 달라지고 싶고 변화하고 싶다는 생각만으로 님은 충분히 지금보다 훨씬 나은 사람이 될거에요. 기대됩니다. 긍정적인 변화는 인생을 순방향으로 이끌어 주는것은 확실하니까요 ',\n",
       " '네이트판에 쓰잘데기 없는글은 읽지말어 남혐 여혐 차별 연예인 비판 등등',\n",
       " '자존감 높이는법',\n",
       " '쓰니 아직 어리지만 사람은 저마다 타고나는 성향이 있어. 남과 다르다고 내가 틀린건 아니야. 다만 타고난 나의 성향은 나의 잘못이 아니며 내가 받아들여야 할 뿐이지. 하지만 내가 받아들인거지 남에게도 이게 나니까 받아들여 라고 할순 없겠지. 그래서 일종의 가면같은 것을 쓰게돼. 어른이 되면 차차 알게될거고 그건 어른스럽다 라고 칭찬받는 순간이 반복되면 뭔지 깨닫게 될거야. 한가지 명심할것은 지나치게 솔직하지말기 마찬가지로 남에게도 그러할것을 요구하지 않기 바라지않기... 나의 삶이 나의 하루 기분이 타인에 의해 곤두박질 치게끔 허락하지 말기 ',\n",
       " '헬스 등록해보는건 어떨까 살빼고 근육붙는 자기자신에게 자신감 뿜뿜 ',\n",
       " '남과 비교 안하면 됩니다',\n",
       " '자존감 말그대로 내가나를존중해주는거야 타인과 나를비교하지말고 나와타인이 다름을인정하고 내가 나를사랑하는거지']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = comment('https://pann.nate.com/talk/349711351')\n",
    "t = []\n",
    "for i in s:\n",
    "    t.append(clean_text(str(i)))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-24a30a64d138>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'https://news.nate.com/view/20200305n25115'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_text' is not defined"
     ]
    }
   ],
   "source": [
    "t = get_text('https://news.nate.com/view/20200305n25115')\n",
    "cl = clean_text(t)\n",
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    wd = \"C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver\"\n",
    "    driver = webdriver.Chrome(wd)\n",
    "    driver.get(url)\n",
    "    pages = 0\n",
    "    try:\n",
    "        while True:\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"#cbox_module > div > div.u_cbox_paginate > a\"))).click()\n",
    "            time.sleep(1.5)\n",
    "            print(pages, end=\" \")\n",
    "            pages+=1\n",
    "    except exceptions.ElementNotVisibleException as e: # 페이지 끝\n",
    "        pass\n",
    "    \n",
    "    except Exception as e: # 다른 예외 발생시 확인\n",
    "        print(e)\n",
    "    html = driver.page_source\n",
    "    dom = BeautifulSoup(html, \"lxml\")\n",
    "    # 댓글이 들어있는 페이지 전체 크롤링\n",
    "    comments_raw = dom.find_all(\"span\", {\"class\" : \"u_cbox_contents\"})\n",
    "    # 댓글의 text만 뽑는다.\n",
    "    comments = [comment.text for comment in comments_raw]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "p=comment('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['의미부여없이 웃기기로는 놀면뭐하니에서 오늘이 젤웃겼음ㅋㅋ지석진 조세호냅두고선 문닫고도망치는거 최고의 킬링포인트ㅋㅋㅋㅋ',\n",
       " '유느 행복해하는거 넘 보기좋네요ㅎㅎㅎ',\n",
       " '오프닝으로 뽕뽑넼ㅋㅋㅋㅋㅋ개웃겨',\n",
       " '유느가 기뻐하면 좋아요😆😆😆😆 토크만으로도 웃김',\n",
       " '유재석 저렇게 즐거워 죽을 거 같은 얼굴로 미친듯이 웃는 거 근 5년 만에 처음 보는 듯. 나머지 셋 보고 웃는 건데 오히려 그 셋은 덤덤하고 혼자 웃겨서 뒤로 넘어가려는 게 너무 웃겨ㅋㅋㅋㅋㅋㅋㅋㅋ',\n",
       " '유재석 방송 아닌 진짜웃는것같아 보는사람도 조았어요. 그리고 실제로도 책마니 읽는다하네요..괜히 1인자가 아님',\n",
       " '런닝맨에서 수년을봤는데도 광수나오니까 뭔가신선함',\n",
       " '소중한 우리 산슬이가 타고있어요 ㅋㅋㅋㅋㅋㅋㅋ',\n",
       " '차타고 이동할때 소중한 유산슬이 타고 있습니다.ㅋㅋ태호PD의 유느님에 대한 애정 보이지 않는 곳에도 흘러 넘쳐 담주도 무도 본방 사수~',\n",
       " '유느 정말 좋아하는게 눈에 보이드라ㅎㅎㅎㅎ']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(comments):\n",
    "    token_sent=[]\n",
    "    for comment in comments:\n",
    "        temp = okt.morphs(comment, stem=True)\n",
    "        token_sent.append(temp)\n",
    "        max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # 상위 35,000개의 단어만 보존\n",
    "    tokenizer.fit_on_texts(token_sent) \n",
    "    token_sent = tokenizer.texts_to_sequences(token_sent)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index)+1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(token_sent, maxlen=max_len)\n",
    "    model = load_model('model11.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    for i in range(len(X_data)):\n",
    "        if(predict[i] == 0):\n",
    "            a=0\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_load(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articeBody'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\t\\t\\t'놀면 뭐하니' 유재석, 절친♥︎들과 남산→서점→방 탈출..포상휴가 '만끽'[종합]\\n\\t\\t\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = soup.select_one('h2.end_tit')\n",
    "    text_r = text.get_text()\n",
    "    return text_r\n",
    "title = get_title('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 놀면 뭐하니 유재석 절친 들과 남산 서점 방 탈출..포상휴가 만끽 종합 '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "def keyword_sentence(url):#키워드 문장 추출\n",
    "    m_text = get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "    r_text = clean_text(m_text)\n",
    "    data = r_text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=11, num_keysents=10)\n",
    "    keyword = list(keywords.keys())\n",
    "    return keyword[0], sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 놀면 뭐하니 방송화면 캡처 김보라 기자 유재석 지석진 이광수 조세호가 포상 휴가를 즐겼다',\n",
       " ' 조세호도 전문적으로 웃음을 드린다 고 소개했는데 유재석이 전문적인 거 치곤 조금 아쉽다 고 하자 조세호는 근근이 밥 벌어 먹고 산다 고 받아쳤다',\n",
       " ' 이광수의 의견에 따라 유재석은 남산 돈가스 집까지 걸어 가자고 했다',\n",
       " ' 이에 조세호는 우리가 만만한 거다 라고 맞아쳐 웃음을 안겼다',\n",
       " ' 조세호와 이광수도 평상시에 오는 건 좋은데 휴가에 여길 오냐 고 했다',\n",
       " ' 아내와 아이와 행복을 위해 달려나가고 있다 고 했고 이광수는 저는 개그맨은 아니지만 주말 예능을 하고 있다 고 말해 웃음을 안겼다',\n",
       " ' 세 사람은 유재석과 단둘이 대 로 떠나는 여행인 줄 알고 왔는데 알고 보니 인이 떠나는 여행이 됐다',\n",
       " ' 우리 넷이 어디 가는 거 처음이다 라고 기뻐하는 유재석',\n",
       " ' 지석진은 이게 이 프로그램의 위엄이다',\n",
       " ' 걷는 걸 좋아한다 며 여행하면서 차 타는 것도 좋지만 걸으며 보는 걸 좋아한다 고 말했다']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword, sents = keyword_sentence(clean)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'캡처'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    global keyword\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    init_word = current_word \n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for k, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + k # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + k # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def clean_sentence(word):\n",
    "    global keyword\n",
    "    max_words = 35000\n",
    "    clean_sen = []\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'k' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-8a187819ae4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclean_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-ba012d1d4d07>\u001b[0m in \u001b[0;36mclean_sentence\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclean_sen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mclean_sen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-00ef959e2e49>\u001b[0m in \u001b[0;36msentence_generation\u001b[1;34m(current_word, n)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mbreak\u001b[0m \u001b[1;31m# 해당 단어가 예측 단어이므로 break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcurrent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_word\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;31m# 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;31m# 예측 단어를 문장에 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# for문이므로 이 행동을 다시 반복\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'k' referenced before assignment"
     ]
    }
   ],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    global word\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)# 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(word):\n",
    "    clean_sen = []\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(model, t, word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
