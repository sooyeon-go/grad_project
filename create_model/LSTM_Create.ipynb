{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37464bitbasecondacd99fe59c5644d9aba961d3dadaa6cca",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyKomoran import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_csv('comment.csv',encoding = 'cp949')\n",
    "comment_data = dic[['comment']] # 선플 cvs 파일에서 comment만"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['정말 노래 대단하네요', '가창력 대단하고.인기도  굉장해요.연예계의 별이 될꺼 같아요', '노래만큼은  정말    넘사벽  맞다.   최고  맞다.  ', '진짜 잘하네용', '노래 정말잘해']\n"
    }
   ],
   "source": [
    "#한글과 공백을 제외하고 모두 제거\n",
    "normalized_text = []\n",
    "for string in comment_data.comment.tolist():\n",
    "    try:\n",
    "        tokens = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣 \\! \\~ \\.]+\", \" \", string) # 문장을 만들 때 문장부호를 사용할 수 있도록 추가해서 학습시킴\n",
    "    except Exception as e:\n",
    "        print(string)\n",
    "        break\n",
    "    normalized_text.append(tokens)\n",
    "print(normalized_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "다니까  내일도 아닌데 다음드라마는 어떻게 쓰려고 저러나 걱정함\n작가 진짜대단해유~우\n작가 천재 리스펙이당께요 잉~~~  대사 하나하나 버릴께 없다 증말\n작가 천재다..... 마지막회까지 완벽하다.... \n작가 필력 최고다!연기자 모두 수고 많으셨어요~\n작가. 연출자. 출연배우들 모두모두에게박수를 보냅니다. 보는내내 행복했습니다 ~~~ \n작가가  대라는게 놀랍다\n작가가 대사를 모두 명대사로 아주 대단하세요 작가님이 최고네요 누구신지는 모르겠지만\n작가가 참 이쁘게 글을 쓰시네요따스해지는 드라마였어요감사합니다\n작가님   연기자분들  모두모두 감사했습니다 \n작가님  계속 파이팅\n작가님  대단하셔오  다음 작품  기대 할게요  그리고  여기 출연하신분들   믿고 보는  배우님들~~\n작가님 마지막도 넘최고 였고 흥식이 와~~정말 소름키치게 연기 잘하더라~~ \n작가님  사람들에 대한 따뜻한 시선 참좋았답니다  세상은  착한 사람들이 이끌어가는게 맞는거죠\n작가님 감사합니다  두달동안 행복했어요~ \n작가님 감사합니다  해피엔드로 마무리지어주셔서  보는내내 행복했어요\n작가님 감사합니다. 대한민국 사람들 가슴깊은곳에 자리잡은 그 깊은 정을 다시 한번 일깨워준 것 같습니다.  극소수인 나쁜 사람들때문에 개개인 스스로가 감정이 매말라가고 냉정해지는 이 시점에서 아 우리는 원래 이렇게 정이 많은 민족이였었어 하고 다시금 알려 주시는 것 같습니다 결국은 쪽수다! 다만 왕따와 조폭등 나쁜일에는 쪽수가 해당되서는 안되겠지요 작가님 대사 하나하나가 정말 주옥같고 되새기게 되네요 간만에 정말 간만에 주말연속극같은 수목 드라마를 만나게 된 것 같아요  년에 한번 나올까 말까네요 매번 바람 불륜 살인 친자확인\n작가님 감사합니다. 이런드라마 보게해주셔서 \n작가님 감사합니다.기적같은 해피엔딩이라 눈물머금고 미소지으며 그리고 박수치며 봤어요.정말 좋은 드라마 였어요.\n작가님 감사합니다.따뜻해요\n작가님 고맙습니다~ 늘 건강하세요~\n작가님 대단하시고 공효진언니가 까불이직접 잡는거 대박~~좋은배우들 다 좋다\n작가님 대사하나하나가 진짜.최고\n작가님 도대체 뭐야 어케 장면마다 명대사들이냐 작가님 미쳤어 \n작가님 이런 드라마 하나만 더 만들어 줘유ㅜㅜ\n작가님 동백이 엄마 살려줘서 감사해요 마지막회가 이리 재미지고 감동적인 드라마는 처음봐유 동백이 땜시 행복했어요\n작가님 두달여동안 행복했어요... 고맙습니다.\n작가님 드라마보는 내내 행복했습니다.이런 좋은 작품 써주셔서 감사합니다ㅜㅜ감독님 스탭분들 연기자분들 다다 고생  많으셨어요어딘가에 있을 동백이들 다 행복하길!!\n작가님 리스펙트    더 이상 완벽할 수 없었던 조연배우들 리스펙트  \n작가님 배우님들 수고하셨고 감사합니다~~\n작가님 뵙고싶네요!!!\n작가님 사랑합니다존경합니다앞으로 좋은 드라마 많이 만들어주세요 \n작가님 역대급 히트!  다음드라마는 어떻하려고..\n작가님 인생명언 많이 남겨 주셨네 오랜만에 너무 재밌게 봤다\n작가님 작두타신후에 글을 쓰신건가..끝까지 너무 최고였다 버리는 배역없이 모두 훌륭하게 그려주심..다음작품에서 봐요 아묻따 본방사수예요ㅠㅠ아쉬워서 어케 보내ㅠㅠ\n작가님 정말 마음 따뜻한 천재이신가요  이런 좋은 드라마 만들어줘서 고마워요배우분들 감독님들 스태프분들 다 감사해요!!\n작가님 정말 존경합니다 푹쉬고 담작품도 기대할께요 그동안 행복했어 ㅠㅠ\n작가님 정말 진심 감사합니다\n작가님 존경하고 사랑합니다 \n작가님 존경해요!!이런 명품 드라마를 만들어주셔서요...엄마로..딸로..한때 연애할 때의 여자로 또 아내로..며느리로..주민으로..국민으로...누군가의 지인으로...정말 많은걸 생각하며 감정이입되서 울고 웃으며 보았습니다.앞으로 작가님 작품을 기다릴것 같아요!!연기해주신 모든 연기자분들께도 감사드립니다!\n작가님 좋은드라마 너무 감사합니다ㅜ\n작가님 진짜 영혼을 갈아 쓴듯.... 매 회 지루한적 없이 레전드 찍은 드라마는 갠적으로 첨이었어요... 울고 웃고 ㅠㅠㅠㅠ진짜 사람사는 얘기들로 가득해서 너무 행복했어요ㅠㅠㅠㅠ 동백꽃 못보내.. 딥디 블루레이 뭐든 살게요\n작가님 천재세요 \n작가님 최고 배우분들도 최고 제 인생 최애 인생 드라마에요감사해요!! \n작가님 최고십니다!!!!! 단짠단짠에 코믹감동스릴러멜로 통째로 갈아넣으심ㅋ\n작가님 최고\n작가님 팬됐어요 덕분에 가을 잘 보냈습니다 ㅠㅠ \n작가님 팬됬어요~~~~!!정말 드라마 잘만들었어요진짜 감사합니다 너무 재밌게 잘봤어요모두수고많으셨습니다 다음작품도 기대할게요~~ \n작가님 피디님 배우님들 캐릭터들..  덕분에 행복했어요\n작가님 필력 진짜  그동안 많은 웃음과 눈물과 위로를 준 드라마를 만들어 주셔서 고맙습니다.\n작가님 해피엔딩 고마워요~오늘도 펑펑울다가 나도모르게 흐뭇하게 입벌리고봤어요~ 저도 쓰러져위급한엄마랑 응급실 나혼자있을때 동백이같았는데 그때생각나서 펑펑울었어요~세상이 살만하다고 힘을주셔서 고마워요~혼자가아니라고 위로해줘서 고마워요~\n작가님! 진심으로 감사합니다.. 웃고 울며 보다가 인생을 알게 됐습니다. 앞으로도 좋은 작품 계속 부탁드립니다.\n작가님  모든 배우분들  스탭분들 너무 고생하셨고 좋은작품 만들어주셔서 감사합니다   이런글 안쓰는데 최고였어요ㅠㅠ 내인생   위 인드에요\n작가님  배우님  감사해요. 모두들 실재하는 나의 이웃 같았어요. 너무나 많이 웃고 울었네요.  나의 아저씨  이후 또 마음을 후벼파는 드라마예요.\n작가님  정말 진짜 산타할아버지가 주는 선물같은 드라마를 써주셨네요 ㅠㅠ 진짜 진짜 잘 봤습니다!!! 동백이한테 꼭 필요한 기적도 주시고  마지막까지 규태와 자영이 꼭 챙겨주셔서 너무 고맙습니다 ㅋㅋㅋ 케백수는 없는 상이라도 만들어서 전 출연진 싹~~~다 상 하나씩 쥐어 주십셩~~~ 정말 정말 잘 봤어요!!!\n작가님  피디님  스태프님들  그리고주조연 모든 배우님들 좋은 드라마 잘 봤습니다.수고 많으셨습니다.\n작가님 고마워요. 저두 혼자 아들 키우는 싱글맘인데 동백이한테 감정이입되서 울고  웃고  위로받으며 드라마를 보고 또 보았네요.드라마보며 어쩜 저리도 싱글맘의 고통과 외로움을 잘아실까 놀라웠네요.그리고 용식이 동백이한테 헌신적인 사랑을 하는 모습을보며 제가 사랑받는 기분으로 설레고 좋았습니다.이제 수요일  목요일만 기다렸는데 앞으로 많이 허전할거같습니다. 앞으로도  사람들 행복하게 해주는 좋은드라마 많이 만들어주세요. 고맙습니다~\n작가님.. 절받으세요... 서사 완벽.. 진짜 떡밥회수 능력과 인물 한명한명 다 챙겨주는 완벽함에 박수를 보냅니다ㅠㅠㅠㅠㅜ 진짜 여운오져ㅠㅠㅠ동백꽃 못보내겠어요ㅠㅠㅠㅠㅠ\n작가님.. 책임져요.. 못빠져나오겠어ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ 내삶의낙이었는데ㅜㅜㅜ 용식이 주접도 이제 못본다니ㅜㅜㅜㅜ 서사 완벽 떡밥회수 완벽 이런 드라마 첨이었어요ㅠㅠㅠ 감사합니다ㅜㅜㅜ\n작가님... 감사합니다.정말 최고였습니다.\n작가님..어디서 나오셨어요. 이건..말이안되죠. 첫회부터 끝까지 버릴게 하나도 없다뇨...공중파에서 이런 드라마를 보다니! 감사드려요. 마지막도 최고였어요.\n작가님~ 쌈마이웨이도 그렇게 글을 따뜻하고 예쁘게 쓰시더니 이번 작품은 더 훌륭합니다~~\n작가님~ 이런 대단한 필력은 어디서 나오시는 건지요   대사 한 마디 한 마디 배우들의 내레이션 너무 좋았어요~\n작가님은 보물같은 사람\n작가님이 끝까지 보여주고 싶었던..   따뜻함.. 보는 내내 따뜻했고  그렇게 살고 싶네요\n작가님이 남자인줄 알았는데... 대초반 여자분이시래요..정말 머리가 멍해졌어요. 이대사의 깊이감이 어떻게 나올수있었는지.\n작가님이 대상 받아야할듯... 진짜 최고의 드라마입니다.\n작가님한테 감사하단 댓은 첨 달아본다.. 첫회부터 마지막회까지 본방사수하며 수 목이 넘 즐거웠고 대사 하나하나 빠뜨릴수없이 다 넘 행복했습니다. 삶의 활력소가 되었었습니다. 감사합니다.\n작가분의 성함과 어떤분일까 하고 처음으로 궁금해지는 드라마였어요ㅠㅠ 최고였답니다\n작가상 베스트커플 여주 남주 아역상 가즈아 \n잘가~~보고싶을거야ㅜㅜ\n재미있고.너무 어제 오늘 눈물흘리면서 봤다ㅠㅠ배우들 연기 짱짱  마지막회 해피엔딩 좋았어요 \n힐링드라마.제가본최고의드라마.울다웃다..ㅜㅜ끝난게너무아쉽네요\n힐링이고 감동입니다!!!\n힘든 인생들에게 보내는 위로같은 드라마.정말 재미도 있고 가슴 뜨겁게 만들었던   최고의 드라마.\nㅠㅠ  울고 웃고 너무행복했어요\n훌륭한 작가님~배역 하나하나 놓치지않고 모두 살려주는 센스~모두를 주인공으로 만든 탄탄한 드라마였네요~\n흠 잡을데가 읍다. 억지스러운거 하나 없고   나도  따순여자가 되보고싶고 누군가에게  착해 보고싶다.\n인스타 들어갔다가 패션 스타일 보고 완전 놀람. 패피임~ 너무 멋진\n회당 재미없었던적이 없어요. 너무너무 즐겁고 행복했습니다.마지막에 동백이 엄마 살려달라고 엄청 빌었는데 ㅎㅎ감사합니다.우리가 사는 세상도 이렇게 다같이 행복해지길요\n명품을 넘어 인생드라마. 미친 작가에 황홀한 연기의 향연.\n후 이제 머보냐 진짜 너무 재밌는 드라마였다\n알럽  덕분에 울고 웃고 감사해여 \n최우수상 가즈아!!!! 진짜 연기 찰떡같이 너무 잘하네요.. 이렇게 연기 잘 하는 배우인지 그동안 몰라뵈서 죄송할정도..\n현실속 동백꽃님들~웃으며 살아요.살다보면 예쁘게 피어날 그날올겁니다.괜찮은 인생이다 느낄날 와요.씩씩하게 힘냅시다!!!!\n환장해유~~짱이었음~ \n행복했어요 \n무뚝뚝연기도 좋았다 ㅎㅎ\n행복하게 마무리 해줘서 너무감사해요ㅜㅜ기적은 평범한 이들이 만드는 소중한 하나하나가 모며서 만들어진다는거..다시 한번 느끼게해주는 드라마였네요..그리고 세상누구보다 엄마의 사랑은 강하다는거 ㅜㅜ\n행복하네요.좋은 드라마 잘봤습니다\n행복한나라였네요~ \n행복해서 눈물이나\n행복했고 행복했습니다.\n행복했어요 감사합니다\n행복했어요~   좋은드라마 감사해요 작가님 배우님 스텝분들...정말 소장하고싶을 정도로  ..  살인생 살면서 처음이야 ㅜㅜ 동백이 볼수있어서 행복했다 증말\n행복은 쫓는게 아니라 음미라는 것이라는 말이내내 마음속에 남을 것 같아요\n행복을 그때 그때 느껴야한다는걸 깨닫게해준 드라마  나중에  행복할거라면서  쉬지않고 달리기보다는 지금의 순간을 감사히생각하면서 행복을  느껴야겠다  용식이같은  사랑  따뜻하고 의지되구  드라마가 정말 행복하다  인생드라마\n행복이 무엇인지 ~ .어떻게 살아가야 하는지를~깨우쳐준 드라마.  행복했고  감사합니다.   소소하고 잔잔한  인간미 넘치는 드라마. 이게 드라마지. 이런 드라마  계속 만들어주세요.폭력적이고  자극적이고  이기적인 드라마에 지쳤고    그런소재를 보고   영향을 받고  또   피폐해져가는   요즘 드라마에  분노합니다.  정말  훌륭한 작가님입니다 \n해피엔딩 동화 본 느낌이다. 작가 잘쓰네.\n해피엔딩 조타조아 따숩고 행보케 정말 ㅎ 고생하셨어요행복했어요 우리\n해피엔딩~ 마음 따뜻한 드라마였다  최고의 드라마!!\n한번도 마지막회까지 흥미진진하며 재밌게 본 드라마가 없이 항상 중간쯤되면 지루해져서 하차 했었는데 인생 유일하게 끝까지 너무 재밌게 본 드라마 진짜 앞으로 이런 드라마 또 없을듯.\n항상 해피엔딩이라 너무좋다 \n해피앤딩 넘넘좋아요 감사합니다 \n한명도 놓칠수 없는 캐릭터와 그들의 대사 ...멋지다멋지다 이런 드라마가 나오다니 ㅠㅠ\n하 진짜 요 몇년 공중파 드라마 다 안봤는데 진짜 얼마만에 드라마 다운 드라마 인가 꿀잼 ㅜㅜ\n하 진짜 인생 드라마ㅠㅠㅠㅠㅠ\n하...이제 뭐 보지ㅜㅜ내사랑ㅜㅜ 동백이만 매주기다렸는데ㅜㅜㅜㅜ진짜끝까지 완벽했어요ㅜㅜㅜ 진짜 사랑합니다. ㅜㅜ\n하~~아~~  엔딩 장면까지 버릴게 없다 없어\n하루에 다몰아볼만큼 정말 지루한 부분이 없다 사랑해요\n한 대사 대사 안에 인생의 현실적 조언아닌 조언으로 다시끔 인생을 보는 기준과 가치관에 대해서 생각나게 해주고 표현의 아름다움에 감사드립니다\n한 몇년만에 본방 재방까지 챙겨본 드라마였어요이젠 무슨낙으로 사나드라마로 행복할 수 있다는 것을 느꼈습니다작가님 배우님들  행복한 드라마 감사합니다\n작가님 드라마 다 좋아했던 드라마~ \n팍팍한 삶 가운데 작은 자양분이 된 동백꽃 늦게 보기 시작해서 몇년만에 다시보기를 시작했네요 대본집 나오면 구매하고 싶습니다 동백꽃 다이어리로 나오면 좋겠습니다 포스터 엽서로 나오면 좋겠어요 오래 기억될 드라마입니다 꼭 소장하고 싶어요\n펑펑 울었다..정말 인생드라마임..해피엔딩 해주셔서ㅜ감사해요 \n태양의후예 다음으로인생최고드라마..아니 마지막회까지 재밌는 드라마는 진짜처음 진짜진짜최고\n투톱 작가인듯. ㅠㅠ 특히 작가님은 나이도 아직  대라던데 어떻게 이런 노련함을 드라마에 녹여내는지 ㅜㅜ  임상춘이라는 이름도 가명으로 쓰는거라던데 아무튼 앞으로 이 작가님꺼 다챙겨볼거임..\n크리스마스는 아직 좀 남았지만.. 웬지 클스마스 선물같은 드라마였음.. 꾸벅  잘 봤어요  출연진 팬됐음 \n크리스마스의 기적 같은 드라마 ㅠㅠ\n캐롤과 함께 내달리는 엠뷸런스.. 쏟아지는 눈발에 내 눈물발도 함께. 마음속 크리스마스였다. 기적이 왔잖아요\n캐스팅 연기력 시나리오 웃음  스릴  가족애   표안남  떡밥전량회수  주인공의 행복. 인생의의미 주옥같은 대사들  마지막회감동 ~~~걍 너무 완벽해 할말을 잃었다  행복했습니다~~\n코믹 액션 로맨스 스릴러 감동 인생 다있었어요. 최고예요 진짜.\n코믹이면 코믹 감동이면 감동멜로면 멜로 재미면 재미 모든 요소갖춘 드라마배우들 한 분 한 분 구멍없이 연기를 너 무 잘하셨음게다가 미친듯한 작가님 필력뭐하나 빠지는 게 없는 드라마는 처음이다.여태껏 아무리 재미있는 드라마봐도채널 돌리는데 돌리지 않았다정 말 간만에 드라마 다운 드라마 봤다.여운이 길게 남을꺼 같다이런 드라마를 만들어주셔서 감사합니다.\n쵝오의드마라  덕분에 행복했습니다 ㅎㅎㅎ\n출연진 모두가 조연이 아닌 주연이다 모두가 불행이라는 환경을 따뜻한 마음과 사랑으로 행복한 삶으로 변화 시키는 모습~~참 좋은 드라마 잘 봤습니다\n칭찬혀 아주 칭찬혀\n최고의 작가님  대사한마디한마디가 다 의미있으면서 과하지않았어요 어떻게 이런 작품을 쓰시는지....감탄하고갑니다\n최고의 작품명작\n최고의드라마.. 보내지못할것같은드라마..처음으로댓글쓰는드라마..재방도 무조건 보고또보게될드라마..감사합니다..수고하셨어요\n최고의드라마 \n최고최고의 드라마입니다\n최고ㅜㅜ\n최곱니다.모두들 수고하셨습니다 덕분에 보는 내내 따뜻했습니다. 작가님!!응원합니다 \n최근 본 드라마 중에 엔딩이 제일 좋았어요\n최고의 드라마 네요 매회 뜨겁게 아프며 슬펐고 또 뜨거운 웃음을 준 드라마 고생들 하셨어요~\n최고의 드라마 만들어주셔서 감사합니다  정말 평생 못 잊을거에요ㅠㅠ\n최고의 드라마 였어요 여운이 오래 남을것 같아요\n최고의 드라마 ㅠㅠ 끝나고도 두번세번 다시 정주행 할겁니다\n최고에 드라마 최고의 배우와 작가 모든분들 오래 기억 할께요\n최고...안 보신 분들 꼭 보세요. 후회 안 하실 겁니다ㅠ\n최고다 모두들 고생하셨어요  잊지못할 인생드라마 \n최고다진짜 연출작가배우 전부싹다\n처음부터 끝까지 완벽한 드라마였습니다\n첫 회부터 이건 봐야겠다.. 하구 한 회도 안 빼고 다 본게 너무 뿌듯하네요. 정말 정말 감사합니다.~ 미친 필력 작가님  연출진  모든 배우님들 감사했습니다. 오늘까지도 울었지만.. 눈물마저 행복했습니다.  잊지 못 할 것 같아요.\n첫회부터 마지막까지 대사 하나 하나 허트로 쓴거 없고 모두가 주연이였던 드라마탄탄한 스토리에  감사합니다..좀 모지리같지만 그냥 오지랖떨래요 저도  보고싶을거에요\n청룡영화제 매번 챙겨봤었는데 이 멋진 드라마 보느라 제꼈다.너무 너무 너무 좋은 드라마였다!\n책으로 나왔으면 좋겠어요.  주옥같은 대사  오래오래 여운이 남을 듯 해요.  작가님 고맙습니다~!\n책으로 내주세요.대본집말고 소설책으로 꼭요 책으로 평생 소장해서 읽고싶어요\n책으로 내주세요ㅜ 정말 너무나 아름다운 시간들 이였어요\n참 잘 만들었다.\n참 잘만든 드라마  \n참 좋은 드라마였다. 누구하나 나무랄꺼없이 다 연기가 최고였더\n짝짝짝.행복했습니다.감사합니다 \n착하고  따뜻한 참 좋은 드라마\n착한 넘은 끝이 없이 빽업이 되야~착한 사람이 당하고만 사는 세상인 줄 알았는데.. 그렇네요 세상에 착한 넘들 천지네유 ㅠㅠ 알려줘서 고맙습니다\n진짜재밌었어요\n진짜저 시간없어서 드라마안보는데 이건 매주본방사수했다. 술마시다가도 이거보면서먹었다.  진짜 작가가 천재인것같다. 모든장면이다이어져....\n진짜진짜재미있게봤어요ㅎㅎ울다웃다가또울다가웃고ㅋㅋ\n진짜한회한회마다눈물안흘린회차가없었네요 ㅜㅠ앞으로 이 드라마작가분 작품완전눈여겨볼듯 ㅜㅠ동백꽃이피워서행복했습니다ㅜㅠ\n진짜  드라마 참 따뜻하다~~고마워요 \n진짜..역대급 탑 안에 드라마였음.. 회부터 본게 뿌듯하다\n진짜뻥안치고 펑펑울었다..진짜 내인생 최고드라마였다..\n진짜 최고네요!!\n진짜 최고의 드라마\n진짜 최고의 드라마 정말 마지막까지 최고였어요!!\n진짜 최고의 힐링 드라마였습니다ㅠㅠㅠ\n진짜 최고ㅠㅠ 조연하나하나가 다 소중해 소중해 마지막화에 반전에 감동에 웃음에 행복에 사랑까지........ 감사합니다\n진짜 특별한 드라마였다.. 잊지 못할것 같음 ㅠㅠ\n진짜 좋은드라마였다.. 정말 최고로 좋은 드라마\n진짜 최고 드라마~작가님  배우님들 감사합니다\n진짜 최고 였어요~~~넘 멋졌다 \n진짜 재밌게 봤어요. 작가님 짱!\n진짜 제 인생 드라마입니다...\n진짜 잘봤던 드라마~~감사합니다!!\n진짜 조연들도 완벽했고 강하늘 공효진 아니었으면 어쨌을까 싶네요마지막회는 특히나 작가님 필력이ㅜㅜ 미쳤다는굿바이 동백데이ㅜㅜ\n진짜 최고 작가님  감독님 최고 저런 모든 배우들이 주연인 저분들을 캐스팅한 감독님도 대박이고 작가님 대사 하나하나 씬이 너무 진짜 제가 드라마 잘 안보는데 이거 정말 완주한 드라마.ㅠ 다시 첨부터 보려고요.ㅠ 주연은 말할거도 없이 조연들까지 다들 연기 대박.ㅠ 너무 수고하셨어요. 한동안 빠져나오지 못할꺼 같아요.ㅠㅠ\n진짜 하 최고에융 두말 필요없어융 다 최고에융 쩌거 작가님 아주 다음작품 목 빠지게 기둘릴꺼에융 쌈 마잉웨이때부터 알아봤슝 작가님 쪼대로하세융 응원해융~ \n진짜 힐링 드라마였어요 !! 보내려니 아쉽네요 ㅠ 마지막 회  마지막 장면까지 버릴게 하나도 없는 따뜻한 드라마였어요 감사해요 ~\n진짜 인생드라마  최고중의 최고 연말시상식 다 휩쓸자 내일부터 다시보기로   또봐야지~~~\n진짜 인생드라마 수고하셨어요 모두다 ㅠㅠㅠㅠ\n진짜 잊지 못해요.. 이리 이쁜커플ㅜㅜ\n진짜 작가님 엄청 섬세하고 꼼꼼하고 철두철미하고 센스넘치는 분 같아요. 처음 부터 끝까지 이렇게 개연성 완벽하고 구성 탄탄하게 이어지는 드라마 처음인 것 같아요. 마지막도 완벽........ 덕분에 많이 울면서 웃었습니다. ㅠㅠㅠ\n진짜 이런 드라마 처음이었다 정말 무해하고  웃고 울게 만든 드라마  끝까지 재밌고  눈물나게 만든 드라마 역대급 정말 최고의 드라마였다\n진짜 이렇게 완벽한 드라마 처음봐요 정말 보는동안 같이 울고 웃고 행복했어요ㅠㅠ\n진짜 올해 최고의 드라마 아니 최고의 작품\n진짜 올해영화드라마 통틀어 최고\n진짜 완벽한 드라마다!! 모두들 상 주고 싶은 드라마는 처음이네ㅎ 그나저나 저렇게 서로 좋아하는 사람 난 언제쯤 다시 만나려나ㅠㅎㅎ\n진짜 이 드라마 보면서 함께 웃고 울고 가슴 아프고 같이따뜻한 사랑을 느꼈던 시간이였습니다 ㅠㅠ 마지막화 역시 대박이였어요 ㅠㅠ 흑   함께해서 행복하고 즐거웠습니다 잊지않을게요 잘가요 동백꽃  \n진짜 이 드라마..  환장해유 ㅜㅜㅜㅜㅜㅜ\n진짜 연기자들은 작가한테 잘해라 . 드라마 원래 안보는데 이드라마는 대사 하나하나가 다 주옥이고 모든 캐릭터를 다 살려줬다. 작가님이 모든 역할들을 진짜 다 사랑한것 같아 시청자인 나도 모두 사랑하는 마음으로 봤다\n진짜 오늘 웃다가 울다가.. 제 인생 드라마 입니다 이런 드라마 만들어주셔서 정말 감사합니다 ㅠㅠ  월에 여기 출연하신분들 다들 상 받으세요 꼭..\n진짜 오랜만에 따뜻했네요\n진짜 미친드라마....후...\n진짜 박수밖에 안나오는 드라마ㅠㅠ최고였어요\n진짜 빨리 끝나기를 기다렸는데 어찌나 질리든지ㅋ\n진짜 마지막까지 최고였어요!!! 너무감동적인드라마\n진짜 마지막회까지.. 이렇게 흥미진진 하다니ㅋㅋㅋㅋㅋㅋㅋㅋㅋ\n진짜 마지막 순간까지 최고였어요!!!!!\n진짜 두고두고 꺼내볼 작품입니다 \n진짜 드라마중 최고\n진짜 따뜻한 글 따듯한 드라마 감사합니다 \n진짜 띵작입니다.. 작가님 정말재밌게 잘봤습니다.감사합니다~\n진짜 마지막 마무리까지 완벽했다구요 ㅠㅠㅠㅠ끝까지 울다 웃다 난리였네요 진짜... 이런 좋은 드라마 써주셔서 너무 감사합니다 정말로..ㅜ\n진짜 마지막까지 따뜻하고 완벽했다 ㅠㅠㅠㅠㅠㅠㅠ이제 보내야하는데 아쉬워ㅠㅠ\n진짜 내 인생 드라마... ㅠㅠ헤어지기 싫다...작가님  얼른 또 이런 작품 써 주세요!\n진짜 너무 좋은 드라마예요.  배우들도 좋았고 그냥 최곱니다 작가님 \n진짜 너무너무 따뜻했던 드라마 \n진짜 더할나위없이 좋았다. 행복했어요.\n진짜  감사합니다~너무 행복했습니다 \n진짜 감사한 드라마ㅜ 배우님 연기 분량이 많지 않아도 나오실때마다 눈물 쏟았습니다\n진짜 결말까지 완벽한 드라마 생에처음이에요\n진심 내인생작\n지상파에서 간만에 멋있고 이쁜  드라마 시청했네 요즘 사회가 각박한데 이드라마에서는 옛날 어릴때 동네사람들 같은 느낌을 느낄수 있는 아름다운드라마였다\n지금까지 이런  엔딩은 없었다작가님과 배우들께 찬사를 보냅니다행복했습니다 ~~~이 드라마 여운으로 앞으로 몇년은 충전되어거뜬히 살아낼것같습니다\n지금까지 이런 글 써본적이 없는데 안쓸수가 없네요 ㅠ 정말 최고의 드라마였어여 ㅠㅠ 주연부터 조연들까지 아니 조연이 없는 듯한 드라마는 처음   그리고 작가님 정말 최고  \n지금까지 이런완벽한 엔딩은없었다!찝찝함은 일도없는 사려깊은 엔딩이었어요.잘봤습니다~\n지금까지 이렇게 완벽한 드라마가 있었던가  처음부터 끝까지 웃음과 눈물 감동을 선사한 내 인생 최고의 드라마였습니다. 작가님  모든 배우님들 연출감독님 및 스탭분들 모두 너무나 감사합니다 \n지금까지도 매회 감동이었는디 마지막회가 제일 잼있다니\n중간부터 보기 시작했는데... 뭐랄까 뭔가 은근히 기대하게 만드는 그리고 보는 내내 희노애락을 느낄 수 있었던 드라마였습니다 그 캐릭터에 담긴 과거모습도 참 좋았습니다 이 드라마를 위해 고생해주신 모든 분들께 감사합니다 덕분에 즐거웠습니다.. \n주연 조연 할 것 없이 다 돋보이고 마지막까지 이렇게 감동적이고 완벽한 드라마 처음 보는것 같아요!! 진짜 최고에요 \n주연은 물론이고 조연 모두 다 매력있고 좋았어요. 엄마  회장님  필구와 베프 준기  향미  규태  자영  옹벤저스  의사쌤  간호사님 까지도 다 따수웠던.. 이런 드라마 또 만날 수 있을까 싶네요\n좋은 드라마 만들어주셔서 정말 감사합니다 \n좋은 드라마 볼수있게 해주셔서 감사합니다 이번엔 처음부터 끝까지 못보고 있었지만요 오늘이 마지막회라 잘 봤다는 남기고 싶어서요 작가님 팬입니다 배우님들 그외 모든분들 수고들 많으셨습니다 다음에도 공중파서 작가님 작품 볼수 있었으면 공중파 방송들 관심 많이 받아야합니다\n좋은 드라마 올해의 드라마 감사합니다\n좋은 드라마 한 편이 우리에게 주는 사랑받고 아낌받는 느낌...슬펐지만 따수웠어요~!\n좋은 드라마였어요\n좋은 드라마와 함께한 시간들 행복했습니다\n좋은 작품 만들어주신 배우님 작가님 스텝분들 정말 감사합니다 마지막까지 잘 봤습니다\n좋은드라마  잘봤어요~ 이젠 뭘보나ㅠ\n좋은드라마 감사해요마지막까지 울고웃기고 감동주고 다하셨어요 최고\n좋았어요 다 좋았어요 고마워요~\n제발 안 보신분들 꼭 보세요 ~!!! 수작입니다~\n제인생 드라마중 역대급 최고의 결말이었어요!!!!작가님 완전 짱!!!\n조연없는 드라마~출연자 모두가 주연이었다  와우~~~\n존잼\n제가 요즘 관심 있어하는 주제를 다루어 너무 좋았어요. 모든 캐릭터들의 대사들이 제 마음 속에 콕콕 와닿았어요. 가족과 그리고 주변사람들을 더 더 사랑하고 싶은 드라맏\n제대로 미친 드라마 이렇게 완벽한 결말은 없었다 연기대상 최초 팀대상 가자!!!\n제대로된 드라마 본거 같아 너무 행복했습니다  마지막회까지 완벽한 드라마였어요. 인생작입니다. ㅜㅜ\n정말간만에..따뜻한드라마잘봤어요현실성있는대사들..배우님들의열정적연기력...감사했습니다\n정말감사했습니다덕분에 행복했습니다\n정말정말 감사했습니다이 드라마 보면서 힘을 얻었습니다지금 저를 힘들게 하는 역경들 때문에삶을 포기할까 생각도 했었는데이겨낸다고 다짐했습니다.다들 화이팅입니다!!!\n정말정말 곱고 예쁜 소설 한 편 본거 같은 기분이었어요... 내내 행복했습니다~ 이제 진짜 끝이라니 마음이 아려요..ㅠㅠ\n정말정말 행복했습니다!!!!!~~~ㅜㅜ\n정말 행복했습니다. 사람사는 세상 많이 따뚯하고 행복했어요~ \n정말 행복혔어요\n정말 현실적이고 잘봤습니다..배우들도 모두 최고입니다..이런드라마는 최고의 드라마입니다..\n정말좋은드라마였어요   이제수목뭔낙으로사나요ㅜ다시보기로계속봐야지\n정말좋은작품 써 주셔서 감사합니다 모든 스텝 배우분들 고생하셨구김선영배우님 정말 인상적이었습니다~\n정말 최최고! 잘나가다가 막방을 막장으로 끝내는 드라마가 대부분인데.. 정말 마지막까지 쫄깃 달달 따뜻 완벽합니다~~~ \n정말 티끌하나 실망스러운거 없이 끝까지   완벽한 드라마는 처음이었다.  년 넘게 모든 드라마들 챙겨 봐온 내인생 최고의 드라마였다.\n정말 평생 갈 인생드라마..감사했습니다 모두\n정말 항복만킥하면서 끝났다~~첫회부터마지막회까지 보는내내 설렜구 행복했당..이제못본다니 다음주부터 용식이없는 일주일이 힘빠질꺼같다..그래도 황용식씨덕분에행복했어요~~ 근디 기자양반..흥식이가흥식이를연행해요 아무리 급해도 수정은해가면서 기사올립시다.문법안맞는 문장들도많고..읽다가짜증나는기사들많음..제대로 국어공부못했나봐\n정말 첫회부터마지막회까지 재미있게 잘봤어요\n정말 최고!!  회마다 가슴에 새겨지는 대사들이 잊혀지질 않을거같아요\n정말 최고였어요 작가님이 얼마나 본인 작품에 애정을 가지고 신경써서 집필하셨는지 느껴졌습니다 감사합니다 잊지 못할 드라마 보여주셔서\n정말 최고였어요~ 끝나서 너무 아쉬워요~ 연말 시상식 완전 기대됩니다!!!\n정말 제  인생. 최고의 드라마였어요몰입해서 보느라 늘 매회마다 시간가는줄 모르고 봤네요작가님 해피앤딩으로 마무리해주시고 너무 감사해요끝까지 진한 여운과 많은걸 생각하게 해주셨어요영원히 잊지않을께요정말 최고였습니다!!!!\n정말 작가님짱  배우님들 짱 대단하고 아름다운 드라마 정말 감동 그자체였습니다~~담주부터 이제  뭐 할까 나ㅠ\n정말 잘봤습니다..배우들도 훌륭했지만 이드라마를 보는내내 작가에대한 감탄을 하게 되드라고요..다음작품 기대할께요..\n정말 재미있고 힐링되는 드라마였어요~ 마지막까지 우리의 삶도 응원해주고..저도 모두의 삶을 응원해주고 싶네요  연말에 상 많이많이 타시길 바래요!!!!\n정말 완벽했던 드라마~~~오랫동안 기억에 남을 드라마~\n정말 완벽한 드라마였어요. 작가님 스탭분들 배우분들 감사합니다 \n정말 완벽한 엔딩\n정말 많이 울다가 미친듯 웃다가  무섭다가  완벽한 엔딩 감사합니다 \n정말 멋진 드라마였어요 감사합니다~ \n정말 보는내내 행복했어요. 이젠 볼수없지만 오래도록 기억에 남을 드라마예요   작가님  연기자분들 정말 행복한 시간이었어요\n정말 보면서 울고 웃고 행복한 드라마였음ㅠ 내 인생 드라마\n정말 완벽했던 드라마. 가슴따듯히 잘 봤습니다. 수고했어요\n정말 드라마를 평소에도 좋아하지만 한 부분도 빠짐없이 끝까지 정주행한 드라마는 오랜만이네요 대사 하나 하나가 모두 주옥 같았고 웃고 울고 힐링 제대로 했네요 오랫동안 기억될 드라마같네요ㅠ 진짜 감사합니다\n정말 뜨신 드라마였습니다!!! \n정말 마음의 위로가 되었어요~감사합니다~작가님  배우님  스텝분들님~앞으로  이런 드라마 계속 계속 만들어주세요~\n정말 마지막까지 최고의 드라마였습니다ㅠㅠ 그동안 수목만 기다리면서 살았는데 이런 좋은 작품 볼 수 있게 해주신 작가님 배우분들 스텝분들 모두 너무 고생많으셨고 감사합니다ㅠㅠㅠㅠ 못잊어 ㅠㅠㅠㅠㅠ\n정말 너무너무 행복했습니다.\n정말 너무너무 행복했어요.ㅜ\n정말 넘 울고 넘 웃고 내가 사랑하는것같은 드라마였어요  재방송 맨날봐도 지겹지가 않아요 정말 모두 수고많으셨습니다  영원히 기억될거예요~~ \n정말 끝인가요  그냥 계속하면 안되나요  못헤어날것같은데... 맘아퍼\n정말 너무 너무 재밌는 사랑 감동 추리역시 끝날때 까지 끝난게 아닌.. 반전 ㅋ이제 무슨 낙으로 살아야 되나요.. \n전편이 다 최고임\n정말  최고의 명작 드라마였어요. 그동안 울고웃고  행복했어요 모든 배우들 스탭들  수고했어요 끝나서 아쉬지만 재방시청은 계속될듯요ㅋ\n정말 간만에 따뜻한 드라마 봤다. 수고 하셨습니다.\n정말 그냥 한시간보고 아무의미없이 지난 드라마가 아닌 한회한회마다 생각하게만들고 여운까지 남는 그런 드라마 만들어주셔서 감사합니다 \n저 이분 인스타보고 깜놀! 너무 미소년에 패피!!!\n저는 무엇보다도 드라마에서 작가님의 따뜻한 성품  성숙한 인격  다른 시선이 느껴졌어요. 이런분의 글이 세상의 미래를 만들어가는 한 힘이되길 간절히 원합니다. 변치 말아주세요. 정말정말 응원합니다. 최고였습니다!!!!\n정말 보면서 행복했어요.사랑 용서 화해 행복을 배우는 값진 시간 만들어주셔서 감사합니다 \n진짜 능력있으면서 이쁘면 너무멋있게 보여요!!\n둥글둥글하게 복스럽게생기면서 오동통 고급지게생겼어\n세상아름답네\n진짜 예쁘고 동안이다 여신미와 귀여움이 다 있어\n좋은사람 만나 행복하세요\n대한민국의 피겨 스케이팅을 빛냈던 선수! 여전히 정말로 예쁘시네요~!! 항상 응원합니다~!! 화이팅~!!\n넘 좋아요~~ \n이쁘다\n너무 이뿨~!!!\n우아하고 아름다운 연아~   우리나라 보배~\n오늘도 예뻐요  \n너무 좋아..\n여전히 김연아~~~ \n이뿌다~\nㅠㅠ 알바할때 눈앞에서 영접했는데 싸인이나 사진 부탁한번 못하고 포커페이스로 있었던게 너무 너무 아쉬워요 ㅠㅠ 항상 응원할게요 언니!!\n여전히 졸귀닷!! 하여튼 그런 생각밖엔 딱히 안 든다는....\n한국을 빛내주셔서 감사합니다~ \n울 연느님 항상 예쁘다.\n아 이뻐 \n이쁜 건 아니라는 사람있는데 생각해보면 이쁜 건 아님.누가 여왕을 이쁘다 어쩌다함 \n세계 최강 미모. \n언니가 사랑해\n여신\n같은 시대에 사는게  너무 영광\n묘하게 이쁘다....\n연느님\n어쩜  \n지금모습그대로  세까지 쭈~~욱\n레전드\n갈수록 더 이쁘네요\n나이는 나보다 어리지만 존경합니다\n솔직히 멋지다\n예쁘네요\n근황 반가워\n이쁘네...\n멋진사람~~ 항상건강하길~~~\n여왕이라니가 퀸엘사 실사판  \n귀여워.... \n여전히 미모 유지하고 계시는 연느  항상 애정하고 응원합니다~\n이쁘다 진짜 늙지를않네ㅋㅋ\n고급스럽다잉 ㅎㅎㅎ\n대한민국의자랑스런~~국민들 가슴에 기쁨과 행복을 안겨준 넘멋진 열정~~감사히기억할께요~ 늘 홧팅!입니다 넘멋져요~ \n가장 완벽한 인간체\n너무 예쁘네.\n대한민국의 역사...손기정 어르신과 같은...\n진짜 곱다 곱게 아름답다\n 대시절 나의 히어로 여왕님\n여전히 예쁘시네요~\n우리 여왕님 우아하고 이쁘고 귀엽고\n퀸연아. 이 세글자면 됨. 더 이상 설명 필요 없음 .\n댓글반응이ㅋㅋㅋ연느님 이미지 진짜 원탑같다..나도 사랑함\n크 여왕님 아름다우시다ㅎㅎ\n훌륭한인재를 티비에서 볼수없다는게 아숩당~~~프로하나 론칭해서 방송으로 나오길 아님 유투브에서라도~~대중속으로좀 나오시길~~\n언제봐도 이뻐요 최고\n피겨여왕. 다시한번보고싶네요\n이쁘긴 예쁘네요\n진짜 예뻐요 .. 초등학생때 늘 우리나라를 대표해서 멋있고 아름다운 모습 보고 정말 좋아했었는데 !! 여전한 꽃미모  앞으로도 좋은 모습 많이 보여주시고 하시는 일 모두 응원하겠습니다 ~~~ 아름다워요 ㅎㅎ!\n언제 봐도 존예..\n엄청 깔끔하고 매력있게 생겼다!!엄청 이뻐!!\n이뿌당\n진짜 이쁘다\n뭘해도 예쁘다 ~~\n왜 얼굴은 나이 안먹음 \n짱 자기 관리  진짜  잘하네  예쁘다\n우리나라의 유일무이한 피겨스케이팅 여제 !! 어쩜 . 검정색 옷과 대비되는 화사한 김연아의 메이크업도 잘 어울리시네요 ㅜㅜ대한민국을 빛내준 감사한 분 ~~항상응원하겠습니다 !! 이제는 본인의삶을 찾아 행복만 하세요 ㅎㅎ\n항상 그대로 그자리에서 후배들을 위해 열심히 살아가는 모습 아름다워요..\n도대체 김연아는 왜 예쁘기까지 함 \n올림픽 성화 점화할때 모습이 아직도 눈에 선하다 하늘에서 내려온 천사인줄~\n이쁘고사랑스럽다\n갈수록 예뻐지는군\n예뻐요~ 늘 응원합니다. 행복하세요 \n너무 이뽀여~~ \n나보다 어리지만 존경해요 \n레전드.... 나보다 훨씬 어리지만 존경합니다!! 당신의 피겨를 사랑하고 잊지 못해요~~~ \n원래예쁜얼굴인데 우리나라를빛내서 더예뻐보임\n예뻐요\n화사하고 이쁘다\n이뽀 ㅠㅠ\n영원한 퀸 \n이뻐이뻐 진짜 공주님같아요\n사랑해요  ㅠㅠㅠㅠㅠㅠㅠ\n세계신기록세우는 연기 다시보는데 ..역시 최고다\n영원한 피겨퀸!!\n완전 최고 뭘 하든 님의 편입니다\n참 언제봐도 그대로다 기품있고 우아함\n내년이 벌써 벤쿠버  주년이라니! 아직도 그때의 감동은 잊지 못한다 사랑해요!!!\n하고 싶은 거 다 하자아 \n뭘하든 무조건 좋다 ~~~ 그리고 권력에 흔들리지 않는 그녀의 소신은 더좋다 \n인간적으로 너무 이쁘다...\n퀸연아 고마워요  꽃길만걷기를 \n연느는 사랑입니다 최고최고\n자꾸 이뻐지네요~ \n자주좀 볼수있으면 좋으련만. \n현실 엘사\n좋은일에 동참하는  \n꺅~~! 넘나 예쁨\n안녕하세요 여왕님 . 동시대에 태어나주신 것만으로도 영광입니다 .\n이렇게라도 보니 반갑네요  \n대한민국 국보\n너무 예뽀용~우리나라에 김연아가 있어서 자랑스럽네요~ \n깨끗하게 이쁘단말은 여기에쓰는듯\n여전히 사랑스런 연느님~ \n언제나 아름다우세요.\n연아선수 능가하는 선수는 당분간없을듯 보이네 너무 아름답고   완벽했던터라...\n항상응원합니다 \n대한민국 국보 호딴말필요없지\n웬만한 연예인들보다 더 연예인 같은 느낌...개존예 실물 진짜 예쁨\n언제나 즐거운 일만 가득했으면..\n소신철학이 있으면서 평범한얼굴은 마냥 이쁘기만한 얼굴보다 훨 독보적이고 빛남\n대한민국 보물 인생이 행복하길\n대한민국을 빛내준  감사한 김연아님 ~~항상응원함니다\n우와~ 여왕님 여신님!!\n곡 소개 꼭 보세요~~보고 들으니 더 좋음ㅠㅠ최고!!\n뮤비봤는데 완전 러블리자체ㅋㅋ\n참 좋 다!!!!!!\n점점 진정 하는일 하마 차칸일하고 연기도 우러나고~~ 쭉~~승승장구해라~ \n너무 예쁘고 노래도 너무 좋아용! 아이유만의 보컬이 곡을 완성하는 느낌인 것 같아용 앞으로도 좋은 노래 많이 들려주셨으면 좋겠네요 노래 많이 들을게용 화이팅\n전곡이 타이틀감..ㅠㅠ\n차트 줄 세우기 진짜 ㅋㅋ 진짜 멋있다 ㅠㅠ\n여러 음원차트에서 줄세우기 하는거 너무 멋있잖아 \n자장가 좋다\n뮤비 너무 이쁨ㅎ\n사랑 가득한 앨범!! 곡이 다 좋아서 계속 듣고 있어요 행복하게 해줘서 고마워요\n노래가 다 너무 좋다\n모든 행보를 응원합니다~! \n항상 응원합니다\n앨범 고마워요~~ 아이유 덕에 전 잘 듣고 있어요~~ 귀 호강~\n전곡이 다 타이틀감!!!!너무 좋아ㅠㅠㅠ\n언젠간 꼭 가고싶은 콘서트..\n노래 너무 좋아요 들을 때마다 너무 공감되고 위로받는 것 같아요 계속 응원하겠습니다.\n이번 곡들 다 좋아\n우리 함께 꽃피우자  나의 시인\n가라  가 뭔뜻이에요 \n가사 다 너무 좋고 위로되고 사랑이 느껴져ㅠㅠ 아이유 음악은 사랑 \n감탄했음 노래 너무 잘불러\n강인한 여자\n무한반복중 ㅠㅠㅠ\n거를 노래가 없어요 너무 좋아ㅠ\n계속 흥해라~~~\n고마워요.  참 좋다.\n고생했다\n고생했어요. 지금 전곡 듣고 있는데 좋네요~ \n그냥 존제자체가 이 세상의 선물...정말 모든 곡들 너무 좋음\n너무 조아  \n너무 좋다 노래 질리지 않아 \n노래 너무조아 ㅠㅠㅠㅠ\n노래 다 너무 좋아요~\n노래 다 좋네\n노래 전곡 다 좋아\n노래 정말 다 좋더라\n노래 좋아요\n노래 좋음\n노래 항상 너무 좋아요 \n노래가 너무좋더라 ... 앨범 통으로 들음\n노래로 행복하게 해줘서 고마워요 항상 응원합니다\n노래해줘서 고마운 가수\n늘 음악으로 힐링시켜준다. ㅜ ㅜ 이번 앨범 너무 조아여 사랑해요  \n늘 좋은 노래 들려줘서 고마워 \n다 잘하는 사람\n다 좋았지만 개인적으로 가장 소장가치가 높은 앨범인 듯. 이거 듣고 다른 앨범 들으면 감회가 새로움.\n덕분에 기분좋게 하루를 시작합니다!\n독보적인 아티스트\n들으면 들을수록 좋다 \n러브포엠 너무 사랑스러운앨범이야 아이유 사랑해\n모든 곡들이 다 너무 좋아ㅜㅜ\n명반 내줘서 고맙고 수고했어요 연말 앞두고 계속 들을 음악생겨서 넘 좋아요\n모든곡이 너무 좋아요~\n버틴 것도 네 덕분이라는 말이 먹먹하네.. 고생해서 새 앨범 내줘서 고마워요 언니가 작사 작곡한 그 사람이 저는 제일 좋아요 항상 응원합니다 사랑해 \n보기만 하여도 듣기만 하여도 기분좋아지는     \n본인은 보라색만 좋아하는거 아니라고 대놓고 파란색 앨범ㅋㅋㅋㅋㅋ 뭘하든 응원해요 파이팅 \n미니앨범 러브포엠 너무 좋아요 행복한 노래듣게 해줘서 고마워요 \n믿고 듣는 아이유\n블루밍 상콤한게 넘죠아 \n수록곡 다 들은 적 처음이야..\n수록곡 전부 다 너무좋다....\n시간에 바깥이랑 블루밍 들었는데 정말 언니는 당연히 너무 이쁘고 뮤비 색감도 너무 이쁘고 노래도 너무 좋아서 자주 듣게 될 것 같아요 이번 공연도 잘 마무리 됐으면 좋겠습니다\n노래 너무 하나 같이 너무 좋네요. 음원 듣고 소장욕구 생겨서 앨범까지 주문했습니다. 요즘 같이 추워지는 시기에 들으면 더 좋을 듯.\n시간의 바깥 듣는데 눈물 나오더라... 진짜 가요계에 없어서는 안 될 존재\n한곡 한곡 보컬에 감탄하면서 듣는중 곡해석력 진짜 미쳤어\n    노래랑 다른 노래들도 다 너무 좋은 것 같아요~!! 아이유 노래 들을때마다 기분이 좋아져요~ ㅎㅎ 아이유 앞으로도 응원할게요~!\n남은 콘서트 투어도 화이팅  \n너무 예쁘다  음색 최고\n아이유 노래는 싹다 좋아 ㅠㅠ\n누님의 앨범은 진짜 다 너무 좋은 것 같아요.. 노래 하나하나에 다 의미가 있는 것 같고 들을 때마다 행복합니다. 앞으로도 좋은 노래 많이 들려주셨으면 좋겠네요 ㅎㅎ!!\n사랑해ㅠㅠㅠ노래 너무 좋아서 미쳐\n신곡 거를 노래가 하나도 없네요\n아이유 앨범은 전곡이 다 좋아\n이번 노래 이렇게 간절히 잘하나요 ㅠ 너무 잘해 ㅠㅠ 비긴어게인 나왔음 좋겠어요ㅠㅠ\n짱 노래 너무 좋아\n참 좋다\n그냥 하나의 신인류같애 아이유는 아이유야 개조아\n뮤지션이지 줄 세워도 누구도 부정 못함\n사기 캐릭  노래도 잘하고 얼굴도 이쁘고 연기도 잘하고\n어쩜 그렇게 이쁘시고 귀여우시고 노래도 잘 부르시는지.. 정말 너무 부럽습니다. 앞으로도 좋은 노래 많이 들려주시면 정말 너무 좋을것같네요. 행복한 일들만 가득하시길 바라겠습니다.\n누가 따라올수 있을까\n좋은 음악 만들어줘서 고마워요~ 러브 포엠 앨범 전곡이 다 너무 좋아요 수고했어요 화이팅\n앨범 노래 하나하나 다 정말 좋아요...ㅠ\n앨범 전곡 모두 좋아요! 이번주 주말 콘서트도 기대할께요\n앨범 좋다. 노래 좋다. 아이유 좋다\n앨범최고\n언니 좋은 댓글만 보세요! 이번 노래도 너무 좋아요 믿듣유! 항상 사랑해요\n언니덕에 귀 힐링해요 ~ 감사해요 !!!\n여자 솔로가수고  또 매년 혹은  년정도로 활동 꾸준히 한다는 점이 대단함.\n역시 앨범에 버릴곡이 없네요\n예뻐요 \n와 역시 지은이다 ㅋㅋㅋ 나올때마다 취저다 정말대단하다\n우리의 작은 시인 지은아. 네가 주는 위로와 사랑의 노래는 항상 내 마음을 따뜻하게 해줘. 앞으로도 지은이의 앞에 파란 장미꽃길이 펼쳐 지기를 \n음악으로 치유받고 음악으로 살아가는 진정한 아티스트 \n이런 다재다능한 연예인이 또 나올 수 있을까 싶다 앨범 미친듯이 좋네\n이번 신곡 전부 다 최고ㅠㅠ징짱 최고\n이번 앨범 곡들 하나하나 전부 주옥 같고 듣기 좋아요 컴백해줘서 너무 고맙고 좋은 목소리 들려줘서 더더욱 고맙네요 응원합니다!\n이번 앨범 너무 좋다\n이번 앨범 역대급인듯. 노래 미쳤음\n이번 앨범 전곡이 타이틀로 내놓아도 손색 없을 정도로 좋아요\n이번 앨범 정말 대박인거같아요ㅠ ㅠ\n이번 앨범이 최고명반이다 ㄷㄷ 노래가 다 타이틀이야 미쳤다..\n이번앨범 다 내취향\n이번앨범도 명반이야..전곡 다 너무 좋아요\n자랑스러운 아티스트\n자장가 너무 좋음 ㅠㅠ 아이유표 발라드\n전곡 다 좋다... 뮤비도 예쁘고 신나고\n전곡 다 좋더라. 반복해서 듣고 있음\n전곡 반복듣기하고있어요 너무 좋아요ㅜ\n전곡이 너무 좋아요... 진짜 믿듣아이유 거를게없음\n전곡이 다 너무 좋다ㅠㅠ\n정말 좋아 \n제가 힘들때 버틸수 있었던 징짱덕분이었어요 \n존경스러운 아티스트\n좋다\n좋아요\n줄세우기 올킬 미쳤다\n줄세우는건 이유가 있음 들어도 들어도 너무 좋다\n지금 자장가 듣고 있는데 너무 좋아요ㅠㅠ\n누나는 노래할때가 진짜 제일 멋있고 예뻐요. 좋은 노래 고마워요.  \n진심으로 아이유라는 아티스트가 좋아요  고등학생딸도 감탄을하며 듣네요~ \n진짜 노래 인정\n진짜 앨범 한곡 한곡 다 주옥같다 ㅠㅠㅠㅠㅠ 사랑해  \n사랑해 ㅠㅠㅠ내 기준 참 명반이야 좋아하는곡 너무 많아쏘\n최고다 진짜\n최고의 가수 .. 언니 음악해줘서 고마워요 ㅜㅜ \n최애가 챗셔랑 팔레트였는데  원픽바뀜 \n축하한다 ~ ~티비에서도 보고 싶어 이제 좀 나와줘 ㅜㅜ\n출근길 기분좋게 시작했어요~  노래 너무좋음 ㅠㅠ\n카페에서 우연히 듣고 너무 좋아서 검색했는데 아이유 노래였어... 우와 계속 취향에 맞네\n항상 그랬지만 아이유 노래는 앨범 통으로 믿고 들어요! 가사도 재치있고 감동적이고 너무 좋네요ㅜㅜ\n최고야 노래 들려줘서 고마워\n진짜 노래좋음ㅎㅎ\n한곡 한곡 다 좋다.. 아이유만의 감성 \n이번에도 역시 갓이유\n이쁘누\n"
    }
   ],
   "source": [
    "comment_data.comment = normalized_text\n",
    "for sentence in comment_data['comment']:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['의','가','이','은','들','는','가수','연기','걍','과','도','를','으로','자','에','와','한','하다','배우','가수','와','배그','박지훈','연우','드라마','작가','노래']#불용어 제거하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmr = Komoran(\"EXP\")\n",
    "X_token=[]\n",
    "for sentence in comment_data['comment']:\n",
    "    temp_X = []\n",
    "    temp_X = kmr.morphes(sentence)\n",
    "    #temp_X = [word for word in temp_X if not word in stopwords] # 불용어 제거\n",
    "    X_token.append(temp_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 20000\n",
    "t = Tokenizer(num_words = max_words) # 상위 20,000개의 단어만 보존\n",
    "t.fit_on_texts(X_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정수 인코딩 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-305ae9f3f6c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0msequence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0msequences\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "sequences = list()\n",
    "\n",
    "for line in comment_data['comment']: # 샘플에 대해서 샘플을 1개씩 가져온다.\n",
    "    encoded = t.texts_to_sequences([line])[0] # 각 샘플에 대한 정수 인코딩\n",
    "    for i in range(1, len(encoded)):\n",
    "        for j in range(i, len(encoded)):\n",
    "            sequence = encoded[i,j+1]\n",
    "            sequences.append(sequence)\n",
    "\n",
    "sequences[:11] # 11개의 샘플 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word={}\n",
    "for key, value in t.word_index.items(): # 인덱스를 단어로 바꾸기 위해 index_to_word를 생성\n",
    "    index_to_word[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "샘플의 최대 길이 : 20\n"
    }
   ],
   "source": [
    "max_len=max(len(l) for l in sequences)\n",
    "print('샘플의 최대 길이 : {}'.format(max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0   63   11]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0  301 2343]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0   63    3]]\n"
    }
   ],
   "source": [
    "sequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n",
    "print(sequences[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X = sequences[:,:-1]\n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(t.word_index) + 1\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문장 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "s/sample - loss: 5.5178 - accuracy: 0.0599\nEpoch 17/200\n7549/7549 [==============================] - 7s 935us/sample - loss: 5.4816 - accuracy: 0.0600\nEpoch 18/200\n7549/7549 [==============================] - 7s 934us/sample - loss: 5.4478 - accuracy: 0.0607\nEpoch 19/200\n7549/7549 [==============================] - 7s 938us/sample - loss: 5.4131 - accuracy: 0.0620\nEpoch 20/200\n7549/7549 [==============================] - 7s 941us/sample - loss: 5.3756 - accuracy: 0.0620\nEpoch 21/200\n7549/7549 [==============================] - 7s 943us/sample - loss: 5.3371 - accuracy: 0.0644\nEpoch 22/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 5.3004 - accuracy: 0.0624\nEpoch 23/200\n7549/7549 [==============================] - 7s 980us/sample - loss: 5.2605 - accuracy: 0.0635\nEpoch 24/200\n7549/7549 [==============================] - 7s 961us/sample - loss: 5.2213 - accuracy: 0.0673\nEpoch 25/200\n7549/7549 [==============================] - 7s 965us/sample - loss: 5.1809 - accuracy: 0.0691\nEpoch 26/200\n7549/7549 [==============================] - 7s 946us/sample - loss: 5.1415 - accuracy: 0.0715\nEpoch 27/200\n7549/7549 [==============================] - 7s 968us/sample - loss: 5.0977 - accuracy: 0.0750\nEpoch 28/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 5.0563 - accuracy: 0.0768\nEpoch 29/200\n7549/7549 [==============================] - 7s 985us/sample - loss: 5.0136 - accuracy: 0.0828\nEpoch 30/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 4.9701 - accuracy: 0.0856\nEpoch 31/200\n7549/7549 [==============================] - 7s 952us/sample - loss: 4.9278 - accuracy: 0.0888\nEpoch 32/200\n7549/7549 [==============================] - 7s 990us/sample - loss: 4.8832 - accuracy: 0.0921\nEpoch 33/200\n7549/7549 [==============================] - 7s 947us/sample - loss: 4.8392 - accuracy: 0.0955\nEpoch 34/200\n7549/7549 [==============================] - 7s 939us/sample - loss: 4.7951 - accuracy: 0.0982\nEpoch 35/200\n7549/7549 [==============================] - 7s 962us/sample - loss: 4.7494 - accuracy: 0.1080\nEpoch 36/200\n7549/7549 [==============================] - 7s 950us/sample - loss: 4.7045 - accuracy: 0.1103\nEpoch 37/200\n7549/7549 [==============================] - 7s 975us/sample - loss: 4.6587 - accuracy: 0.1163\nEpoch 38/200\n7549/7549 [==============================] - 7s 975us/sample - loss: 4.6155 - accuracy: 0.1192\nEpoch 39/200\n7549/7549 [==============================] - 7s 961us/sample - loss: 4.5696 - accuracy: 0.1244\nEpoch 40/200\n7549/7549 [==============================] - 7s 959us/sample - loss: 4.5245 - accuracy: 0.1294\nEpoch 41/200\n7549/7549 [==============================] - 7s 947us/sample - loss: 4.4801 - accuracy: 0.1355\nEpoch 42/200\n7549/7549 [==============================] - 7s 971us/sample - loss: 4.4380 - accuracy: 0.1454\nEpoch 43/200\n7549/7549 [==============================] - 7s 953us/sample - loss: 4.3962 - accuracy: 0.1460\nEpoch 44/200\n7549/7549 [==============================] - 7s 965us/sample - loss: 4.3509 - accuracy: 0.1546\nEpoch 45/200\n7549/7549 [==============================] - 7s 967us/sample - loss: 4.3107 - accuracy: 0.1617\nEpoch 46/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 4.2690 - accuracy: 0.1670\nEpoch 47/200\n7549/7549 [==============================] - 7s 950us/sample - loss: 4.2248 - accuracy: 0.1714\nEpoch 48/200\n7549/7549 [==============================] - 7s 949us/sample - loss: 4.1840 - accuracy: 0.1760\nEpoch 49/200\n7549/7549 [==============================] - 7s 984us/sample - loss: 4.1426 - accuracy: 0.1816\nEpoch 50/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 4.1049 - accuracy: 0.1876\nEpoch 51/200\n7549/7549 [==============================] - 7s 958us/sample - loss: 4.0640 - accuracy: 0.1961\nEpoch 52/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 4.0249 - accuracy: 0.1987\nEpoch 53/200\n7549/7549 [==============================] - 7s 955us/sample - loss: 3.9882 - accuracy: 0.2053\nEpoch 54/200\n7549/7549 [==============================] - 7s 957us/sample - loss: 3.9496 - accuracy: 0.2098\nEpoch 55/200\n7549/7549 [==============================] - 7s 976us/sample - loss: 3.9096 - accuracy: 0.2235\nEpoch 56/200\n7549/7549 [==============================] - 7s 963us/sample - loss: 3.8746 - accuracy: 0.2249\nEpoch 57/200\n7549/7549 [==============================] - 7s 962us/sample - loss: 3.8359 - accuracy: 0.2316\nEpoch 58/200\n7549/7549 [==============================] - 7s 962us/sample - loss: 3.7966 - accuracy: 0.2362\nEpoch 59/200\n7549/7549 [==============================] - 7s 975us/sample - loss: 3.7626 - accuracy: 0.2428\nEpoch 60/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 3.7266 - accuracy: 0.2530\nEpoch 61/200\n7549/7549 [==============================] - 7s 983us/sample - loss: 3.6891 - accuracy: 0.2583\nEpoch 62/200\n7549/7549 [==============================] - 7s 972us/sample - loss: 3.6579 - accuracy: 0.2603\nEpoch 63/200\n7549/7549 [==============================] - 7s 971us/sample - loss: 3.6203 - accuracy: 0.2698\nEpoch 64/200\n7549/7549 [==============================] - 7s 978us/sample - loss: 3.5865 - accuracy: 0.2738\nEpoch 65/200\n7549/7549 [==============================] - 7s 987us/sample - loss: 3.5520 - accuracy: 0.2790\nEpoch 66/200\n7549/7549 [==============================] - 7s 982us/sample - loss: 3.5166 - accuracy: 0.2871\nEpoch 67/200\n7549/7549 [==============================] - 7s 971us/sample - loss: 3.4853 - accuracy: 0.2932\nEpoch 68/200\n7549/7549 [==============================] - 7s 967us/sample - loss: 3.4507 - accuracy: 0.3012\nEpoch 69/200\n7549/7549 [==============================] - 7s 986us/sample - loss: 3.4183 - accuracy: 0.3069\nEpoch 70/200\n7549/7549 [==============================] - 7s 961us/sample - loss: 3.3864 - accuracy: 0.3104\nEpoch 71/200\n7549/7549 [==============================] - 7s 969us/sample - loss: 3.3539 - accuracy: 0.3157\nEpoch 72/200\n7549/7549 [==============================] - 7s 951us/sample - loss: 3.3249 - accuracy: 0.3216\nEpoch 73/200\n7549/7549 [==============================] - 7s 961us/sample - loss: 3.2918 - accuracy: 0.3276\nEpoch 74/200\n7549/7549 [==============================] - 7s 956us/sample - loss: 3.2576 - accuracy: 0.3309\nEpoch 75/200\n7549/7549 [==============================] - 7s 943us/sample - loss: 3.2285 - accuracy: 0.3415\nEpoch 76/200\n7549/7549 [==============================] - 7s 958us/sample - loss: 3.2024 - accuracy: 0.3428\nEpoch 77/200\n7549/7549 [==============================] - 7s 960us/sample - loss: 3.1705 - accuracy: 0.3520\nEpoch 78/200\n7549/7549 [==============================] - 7s 971us/sample - loss: 3.1414 - accuracy: 0.3579\nEpoch 79/200\n7549/7549 [==============================] - 7s 983us/sample - loss: 3.1142 - accuracy: 0.3600\nEpoch 80/200\n7549/7549 [==============================] - 7s 954us/sample - loss: 3.0829 - accuracy: 0.3683\nEpoch 81/200\n7549/7549 [==============================] - 7s 959us/sample - loss: 3.0543 - accuracy: 0.3749\nEpoch 82/200\n7549/7549 [==============================] - 7s 964us/sample - loss: 3.0287 - accuracy: 0.3797\nEpoch 83/200\n7549/7549 [==============================] - 7s 950us/sample - loss: 2.9990 - accuracy: 0.3840\nEpoch 84/200\n7549/7549 [==============================] - 7s 960us/sample - loss: 2.9727 - accuracy: 0.3889\nEpoch 85/200\n7549/7549 [==============================] - 7s 967us/sample - loss: 2.9483 - accuracy: 0.3946\nEpoch 86/200\n7549/7549 [==============================] - 7s 976us/sample - loss: 2.9193 - accuracy: 0.4012\nEpoch 87/200\n7549/7549 [==============================] - 7s 966us/sample - loss: 2.8939 - accuracy: 0.4032\nEpoch 88/200\n7549/7549 [==============================] - 7s 964us/sample - loss: 2.8672 - accuracy: 0.4103\nEpoch 89/200\n7549/7549 [==============================] - 7s 951us/sample - loss: 2.8441 - accuracy: 0.4130\nEpoch 90/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 2.8199 - accuracy: 0.4177\nEpoch 91/200\n7549/7549 [==============================] - 7s 959us/sample - loss: 2.7944 - accuracy: 0.4230\nEpoch 92/200\n7549/7549 [==============================] - 7s 969us/sample - loss: 2.7719 - accuracy: 0.4254\nEpoch 93/200\n7549/7549 [==============================] - 7s 933us/sample - loss: 2.7467 - accuracy: 0.4285\nEpoch 94/200\n7549/7549 [==============================] - 7s 947us/sample - loss: 2.7237 - accuracy: 0.4375\nEpoch 95/200\n7549/7549 [==============================] - 7s 966us/sample - loss: 2.7006 - accuracy: 0.4374\nEpoch 96/200\n7549/7549 [==============================] - 7s 969us/sample - loss: 2.6763 - accuracy: 0.4467\nEpoch 97/200\n7549/7549 [==============================] - 7s 975us/sample - loss: 2.6544 - accuracy: 0.4455\nEpoch 98/200\n7549/7549 [==============================] - 7s 968us/sample - loss: 2.6317 - accuracy: 0.4507\nEpoch 99/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 2.6108 - accuracy: 0.4524\nEpoch 100/200\n7549/7549 [==============================] - 7s 974us/sample - loss: 2.5916 - accuracy: 0.4573\nEpoch 101/200\n7549/7549 [==============================] - 7s 968us/sample - loss: 2.5695 - accuracy: 0.4598\nEpoch 102/200\n7549/7549 [==============================] - 7s 964us/sample - loss: 2.5477 - accuracy: 0.4672\nEpoch 103/200\n7549/7549 [==============================] - 8s 994us/sample - loss: 2.5272 - accuracy: 0.4671\nEpoch 104/200\n7549/7549 [==============================] - 7s 967us/sample - loss: 2.5063 - accuracy: 0.4760\nEpoch 105/200\n7549/7549 [==============================] - 7s 965us/sample - loss: 2.4870 - accuracy: 0.4809\nEpoch 106/200\n7549/7549 [==============================] - 7s 957us/sample - loss: 2.4673 - accuracy: 0.4821\nEpoch 107/200\n7549/7549 [==============================] - 7s 965us/sample - loss: 2.4475 - accuracy: 0.4867\nEpoch 108/200\n7549/7549 [==============================] - 7s 956us/sample - loss: 2.4310 - accuracy: 0.4866\nEpoch 109/200\n7549/7549 [==============================] - 7s 963us/sample - loss: 2.4119 - accuracy: 0.4928\nEpoch 110/200\n7549/7549 [==============================] - 7s 992us/sample - loss: 2.3936 - accuracy: 0.4948\nEpoch 111/200\n7549/7549 [==============================] - 7s 982us/sample - loss: 2.3737 - accuracy: 0.4978\nEpoch 112/200\n7549/7549 [==============================] - 7s 977us/sample - loss: 2.3576 - accuracy: 0.5013\nEpoch 113/200\n7549/7549 [==============================] - 7s 950us/sample - loss: 2.3384 - accuracy: 0.5058\nEpoch 114/200\n7549/7549 [==============================] - 7s 962us/sample - loss: 2.3221 - accuracy: 0.5112\nEpoch 115/200\n7549/7549 [==============================] - 7s 955us/sample - loss: 2.3043 - accuracy: 0.5115\nEpoch 116/200\n7549/7549 [==============================] - 7s 958us/sample - loss: 2.2891 - accuracy: 0.5168\nEpoch 117/200\n7549/7549 [==============================] - 7s 979us/sample - loss: 2.2717 - accuracy: 0.5186\nEpoch 118/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 2.2538 - accuracy: 0.5203\nEpoch 119/200\n7549/7549 [==============================] - 8s 994us/sample - loss: 2.2377 - accuracy: 0.5243\nEpoch 120/200\n7549/7549 [==============================] - 7s 983us/sample - loss: 2.2222 - accuracy: 0.5262\nEpoch 121/200\n7549/7549 [==============================] - 7s 967us/sample - loss: 2.2075 - accuracy: 0.5297\nEpoch 122/200\n7549/7549 [==============================] - 7s 970us/sample - loss: 2.1908 - accuracy: 0.5313\nEpoch 123/200\n7549/7549 [==============================] - 7s 960us/sample - loss: 2.1772 - accuracy: 0.5332\nEpoch 124/200\n7549/7549 [==============================] - 7s 979us/sample - loss: 2.1604 - accuracy: 0.5369\nEpoch 125/200\n7549/7549 [==============================] - 7s 970us/sample - loss: 2.1484 - accuracy: 0.5368\nEpoch 126/200\n7549/7549 [==============================] - 7s 958us/sample - loss: 2.1344 - accuracy: 0.5406\nEpoch 127/200\n7549/7549 [==============================] - 7s 974us/sample - loss: 2.1172 - accuracy: 0.5444\nEpoch 128/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 2.1032 - accuracy: 0.5468\nEpoch 129/200\n7549/7549 [==============================] - 7s 985us/sample - loss: 2.0909 - accuracy: 0.5495\nEpoch 130/200\n7549/7549 [==============================] - 7s 968us/sample - loss: 2.0779 - accuracy: 0.5497\nEpoch 131/200\n7549/7549 [==============================] - 8s 998us/sample - loss: 2.0614 - accuracy: 0.5527\nEpoch 132/200\n7549/7549 [==============================] - 7s 984us/sample - loss: 2.0492 - accuracy: 0.5532\nEpoch 133/200\n7549/7549 [==============================] - 7s 962us/sample - loss: 2.0366 - accuracy: 0.5549\nEpoch 134/200\n7549/7549 [==============================] - 7s 963us/sample - loss: 2.0240 - accuracy: 0.5594\nEpoch 135/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 2.0092 - accuracy: 0.5629\nEpoch 136/200\n7549/7549 [==============================] - 7s 966us/sample - loss: 1.9988 - accuracy: 0.5634\nEpoch 137/200\n7549/7549 [==============================] - 7s 989us/sample - loss: 1.9847 - accuracy: 0.5686\nEpoch 138/200\n7549/7549 [==============================] - 7s 993us/sample - loss: 1.9720 - accuracy: 0.5674\nEpoch 139/200\n7549/7549 [==============================] - 7s 987us/sample - loss: 1.9611 - accuracy: 0.5719\nEpoch 140/200\n7549/7549 [==============================] - 7s 977us/sample - loss: 1.9508 - accuracy: 0.5716\nEpoch 141/200\n7549/7549 [==============================] - 7s 979us/sample - loss: 1.9390 - accuracy: 0.5725\nEpoch 142/200\n7549/7549 [==============================] - 7s 989us/sample - loss: 1.9286 - accuracy: 0.5753\nEpoch 143/200\n7549/7549 [==============================] - 7s 974us/sample - loss: 1.9147 - accuracy: 0.5772\nEpoch 144/200\n7549/7549 [==============================] - 7s 970us/sample - loss: 1.9042 - accuracy: 0.5784\nEpoch 145/200\n7549/7549 [==============================] - 7s 987us/sample - loss: 1.8948 - accuracy: 0.5827\nEpoch 146/200\n7549/7549 [==============================] - 7s 992us/sample - loss: 1.8825 - accuracy: 0.5854\nEpoch 147/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 1.8727 - accuracy: 0.5841\nEpoch 148/200\n7549/7549 [==============================] - 7s 971us/sample - loss: 1.8631 - accuracy: 0.5856\nEpoch 149/200\n7549/7549 [==============================] - 7s 985us/sample - loss: 1.8523 - accuracy: 0.5855\nEpoch 150/200\n7549/7549 [==============================] - 7s 975us/sample - loss: 1.8419 - accuracy: 0.5903\nEpoch 151/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 1.8329 - accuracy: 0.5925\nEpoch 152/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 1.8222 - accuracy: 0.5916\nEpoch 153/200\n7549/7549 [==============================] - 8s 997us/sample - loss: 1.8108 - accuracy: 0.5917\nEpoch 154/200\n7549/7549 [==============================] - 8s 999us/sample - loss: 1.8031 - accuracy: 0.5965\nEpoch 155/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 1.7926 - accuracy: 0.5974\nEpoch 156/200\n7549/7549 [==============================] - 8s 995us/sample - loss: 1.7831 - accuracy: 0.6013\nEpoch 157/200\n7549/7549 [==============================] - 7s 980us/sample - loss: 1.7755 - accuracy: 0.6005\nEpoch 158/200\n7549/7549 [==============================] - 7s 986us/sample - loss: 1.7642 - accuracy: 0.6037\nEpoch 159/200\n7549/7549 [==============================] - 7s 967us/sample - loss: 1.7559 - accuracy: 0.6038\nEpoch 160/200\n7549/7549 [==============================] - 7s 990us/sample - loss: 1.7475 - accuracy: 0.6056\nEpoch 161/200\n7549/7549 [==============================] - 8s 999us/sample - loss: 1.7405 - accuracy: 0.6074\nEpoch 162/200\n7549/7549 [==============================] - 7s 972us/sample - loss: 1.7317 - accuracy: 0.6087\nEpoch 163/200\n7549/7549 [==============================] - 7s 983us/sample - loss: 1.7319 - accuracy: 0.6042\nEpoch 164/200\n7549/7549 [==============================] - 7s 975us/sample - loss: 1.7153 - accuracy: 0.6100\nEpoch 165/200\n7549/7549 [==============================] - 7s 980us/sample - loss: 1.7069 - accuracy: 0.6117\nEpoch 166/200\n7549/7549 [==============================] - 7s 986us/sample - loss: 1.6967 - accuracy: 0.6107\nEpoch 167/200\n7549/7549 [==============================] - 7s 970us/sample - loss: 1.6908 - accuracy: 0.6139\nEpoch 168/200\n7549/7549 [==============================] - 7s 982us/sample - loss: 1.6827 - accuracy: 0.6139\nEpoch 169/200\n7549/7549 [==============================] - 7s 987us/sample - loss: 1.6763 - accuracy: 0.6131\nEpoch 170/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 1.6680 - accuracy: 0.6177\nEpoch 171/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 1.6617 - accuracy: 0.6164\nEpoch 172/200\n7549/7549 [==============================] - 7s 973us/sample - loss: 1.6522 - accuracy: 0.6149\nEpoch 173/200\n7549/7549 [==============================] - 7s 978us/sample - loss: 1.6469 - accuracy: 0.6178\nEpoch 174/200\n7549/7549 [==============================] - 7s 985us/sample - loss: 1.6396 - accuracy: 0.6197\nEpoch 175/200\n7549/7549 [==============================] - 7s 979us/sample - loss: 1.6343 - accuracy: 0.6192\nEpoch 176/200\n7549/7549 [==============================] - 7s 989us/sample - loss: 1.6255 - accuracy: 0.6201\nEpoch 177/200\n7549/7549 [==============================] - 7s 986us/sample - loss: 1.6214 - accuracy: 0.6206\nEpoch 178/200\n7549/7549 [==============================] - 7s 992us/sample - loss: 1.6129 - accuracy: 0.6233\nEpoch 179/200\n7549/7549 [==============================] - 7s 983us/sample - loss: 1.6070 - accuracy: 0.6245\nEpoch 180/200\n7549/7549 [==============================] - 7s 993us/sample - loss: 1.6009 - accuracy: 0.6262\nEpoch 181/200\n7549/7549 [==============================] - 7s 990us/sample - loss: 1.5934 - accuracy: 0.6245\nEpoch 182/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 1.5858 - accuracy: 0.6256\nEpoch 183/200\n7549/7549 [==============================] - 7s 972us/sample - loss: 1.5825 - accuracy: 0.6259\nEpoch 184/200\n7549/7549 [==============================] - 7s 981us/sample - loss: 1.5759 - accuracy: 0.6270\nEpoch 185/200\n7549/7549 [==============================] - 8s 1ms/sample - loss: 1.5683 - accuracy: 0.6301\nEpoch 186/200\n7549/7549 [==============================] - 7s 988us/sample - loss: 1.5628 - accuracy: 0.6290\nEpoch 187/200\n7549/7549 [==============================] - 8s 995us/sample - loss: 1.5570 - accuracy: 0.6327\nEpoch 188/200\n7549/7549 [==============================] - 7s 990us/sample - loss: 1.5515 - accuracy: 0.6279\nEpoch 189/200\n7549/7549 [==============================] - 8s 998us/sample - loss: 1.5493 - accuracy: 0.6335\nEpoch 190/200\n7549/7549 [==============================] - 7s 982us/sample - loss: 1.5483 - accuracy: 0.6299\nEpoch 191/200\n7549/7549 [==============================] - 7s 926us/sample - loss: 1.5405 - accuracy: 0.6332\nEpoch 192/200\n7549/7549 [==============================] - 7s 934us/sample - loss: 1.5319 - accuracy: 0.6315\nEpoch 193/200\n7549/7549 [==============================] - 7s 924us/sample - loss: 1.5241 - accuracy: 0.6331\nEpoch 194/200\n7549/7549 [==============================] - 7s 925us/sample - loss: 1.5198 - accuracy: 0.6361\nEpoch 195/200\n7549/7549 [==============================] - 7s 933us/sample - loss: 1.5150 - accuracy: 0.6329\nEpoch 196/200\n7549/7549 [==============================] - 7s 934us/sample - loss: 1.5095 - accuracy: 0.6337\nEpoch 197/200\n7549/7549 [==============================] - 7s 929us/sample - loss: 1.5041 - accuracy: 0.6353\nEpoch 198/200\n7549/7549 [==============================] - 7s 926us/sample - loss: 1.5008 - accuracy: 0.6376\nEpoch 199/200\n7549/7549 [==============================] - 7s 928us/sample - loss: 1.4956 - accuracy: 0.6329\nEpoch 200/200\n7549/7549 [==============================] - 7s 938us/sample - loss: 1.4909 - accuracy: 0.6358\n"
    },
    {
     "data": {
      "text/plain": "<tensorflow.python.keras.callbacks.History at 0x1e7dfe8e608>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=max_len-1))\n",
    "# y데이터를 분리하였으므로 이제 X데이터의 길이는 기존 데이터의 길이 - 1\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=19, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "정말 사람 우리나라 구성 드라마 최고 고고 계속 잘 정말 정말 정말 잘 꽃길 밖 전부 고맙습니다 고맙습니다 대사 우리나라 드라마\n"
    }
   ],
   "source": [
    "print(sentence_generation(model, t, '정말', 20))\n",
    "# 임의의 단어에 대해서 10개의 단어를 추가 생성"
   ]
  }
 ]
}