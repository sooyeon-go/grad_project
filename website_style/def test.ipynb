{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.nate.com/view/20200409n03395?mid=n1008'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='realArtcContents'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£\\.]+\", \" \", str(text))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'html.parser', from_encoding='utf-8')\n",
    "    text = soup.find('h3',class_='articleSubecjt')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    driver = webdriver.Chrome(wd)\n",
    "    url_nums = re.findall(\"\\d+\", url)\n",
    "    addr = 'http://comm.news.nate.com/Comment/ArticleComment/List?artc_sq='+url_nums[0]+'n'+url_nums[1] # ëŒ“ê¸€ì°½ ì£¼ì†Œ\n",
    "    driver.get(addr)\n",
    "    pages = 2\n",
    "    try:\n",
    "        for i in range(5):\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"body > div.commentBox.reply_hide > div:nth-child(3) > div.paging_wrap > a:nth-child(\"+str(pages)+\")\"))).click()\n",
    "            time.sleep(2)\n",
    "            print(pages, end=\" \")\n",
    "            pages+=1\n",
    "    except exceptions.ElementNotVisibleException as e: # í˜ì´ì§€ ë\n",
    "        pass\n",
    "    except Exception as e: # ë‹¤ë¥¸ ì˜ˆì™¸ ë°œìƒì‹œ í™•ì¸\n",
    "        print(e)\n",
    "    html = driver.page_source\n",
    "    dom = BeautifulSoup(html, \"lxml\")\n",
    "    # ëŒ“ê¸€ì´ ë“¤ì–´ìˆëŠ” í˜ì´ì§€ ì „ì²´ í¬ë¡¤ë§\n",
    "    comments_raw = dom.find_all('dd',{'class':'usertxt'})\n",
    "    # ëŒ“ê¸€ì˜ textë§Œ ë½‘ëŠ”ë‹¤.\n",
    "    comments = [comment.text.strip() for comment in comments_raw]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 4 5 6 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì–´ì°¨í”¼ 3ê°œì›”ì€ ë¬´ìŠ¨ ì´ë³„í›„ í•œë‹¬ë„ ëª»ê°€ ì´ë‚¨ì ì €ë‚¨ì ê°ˆì•„íƒ€ë©´ì„œ ê³¨ê³ ë£¨ ë°•íˆë©´ì„œ ì‚´ì•„ë†“ê³  ì–¸ì œë¶€í„° ê·¸ë ‡ê²Œ ìˆœê²°í•˜ì…¨ë‹¤ê³  ê¼´ë‘ ë™ê±°3ê°œì›”ê°€ì§€ê³  ë”´ì§€ë¥¼ê±°ì‹¬? ì•„ ì™€ê¾¸ë•œì— ìì²´ ë¹„ì—°ì•  ì„ ì–¸í•˜ì‹  ê·¸ìª½ ì„±í–¥ì´ì‹ ê°€?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = comment(url)\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ì—‘ìŠ¤í¬ì¸ ë‰´ìŠ¤ ê¹€ë¯¸ì§€ ê¸°ì íŠ¸ë¡œíŠ¸ê°€ìˆ˜ ì„ì˜ì›… ì˜íƒ ì´ì°¬ì› ì¥ë¯¼í˜¸ê°€ ê²°í˜¼ ê³„íšê¹Œì§€ ë°íˆëŠ” í† í¬ë¡œ ë˜ í•œë²ˆ ë¼ë””ì˜¤ìŠ¤íƒ€ ë¥¼ ë“¤ì©ì´ê²Œ í–ˆë‹¤. ì¼ ë°©ì†¡ëœ ë¼ë””ì˜¤ìŠ¤íƒ€ ì—ëŠ” ì˜¤ëŠ˜ì€ ë¯¸ìŠ¤í„°íŠ¸ë¡¯ íŠ¹ì§‘ ë¶€ë¡œ ì§€ë‚œì£¼ì— ì´ì–´ ì„ì˜ì›… ì˜íƒ ì´ì°¬ì› ì¥ë¯¼í˜¸ì™€ ìŠ¤í˜ì…œ í™ì§„ì˜ì´ í•¨ê»˜í–ˆë‹¤. ì´ë‚  ì„ì˜ì›…ì€ ê²°í˜¼ì„ í•˜ê³  ì‹¶ë‹¤ ë©° êµ¬ì²´ì ì¸ ì¡°ê±´ì„ ë°í˜€ ëˆˆê¸¸ì„ ëŒì—ˆë‹¤. ì„ì˜ì›…ì€ ì—°ì• ë¥¼ ìµœì†Œ ë…„ ì •ë„ í•˜ê³  ì‹¶ê³  ê°œì›” ì •ë„ ê°™ì´ ì‚´ì•„ë´ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤ ë©° í•œ ë²ˆ ê²°í˜¼í•˜ë©´ ì­‰ ì˜¤ë˜ í•˜ê³  ì‹¶ì€ ë§ˆìŒì´ ìˆë‹¤ ê³  ë§í–ˆë‹¤. ì„ì˜ì›…ì€ ë™ê±° ì´ì•¼ê¸°ë¥¼ í•˜ë©° ë¨¸ë­‡ê±°ë¦¬ë©° ëˆˆì¹˜ë¥¼ ë³´ëŠ” ëª¨ìŠµìœ¼ë¡œ ê·€ì—¬ìš´ ë§¤ë ¥ì„ ì–´í•„í–ˆë‹¤. ë“¤ ì—­ì‹œ ê·¸ê²Œ í˜„ëª…í•œ ê²ƒ ì´ë¼ë©° ì‘ì›í–ˆë‹¤. ë°˜ë©´ ì˜íƒì€ ë¼ë””ì˜¤ ì¸í„°ë·° ë‹¹ì‹œ ì–¼ë–¨ê²°ì— ë¹„í˜¼ ì„ ì–¸ì„ í–ˆë‹¤ë©° ì£¼ë³€ì— ìë¬¸ì„ í•´ë´¤ëŠ”ë° íŒ¬ë“¤ê³¼ì˜ ì˜ë¦¬ë¥¼ ì§€í‚¤ê¸° ìœ„í•´ ë…„ì€ ìˆì–´ì•¼ í•˜ì§€ ì•Šê² ëƒëŠ” ë‹µì„ ë“¤ì—ˆë‹¤ ë©° ì§€ê¸ˆì€ ì—´ì‹¬íˆ ì¼í•  ê²ƒ ì´ë¼ê³  ë§í–ˆë‹¤. ë˜ ì¥ë¯¼í˜¸ë¥¼ ê°€ë¦¬í‚¤ë©° ë…„ì€ ê´œì°®ë‹¤. ë¯¼í˜¸ í˜•ë„ ìˆìœ¼ë‹ˆ ë¼ê³  ë§í•´ ì›ƒìŒì„ ìì•„ëƒˆë‹¤. ì˜¬í•´ ë‚˜ì´ ì„¸ì¸ ì¥ë¯¼í˜¸ëŠ” ì—°ê´€ê²€ìƒ‰ì–´ê°€ ìœ ë¶€ë‚¨ ê²°í˜¼ ì´ì—ˆë‹¤ ë©° ë¯¸ìŠ¤í„°íŠ¸ë¡¯ ì´ ëë‚˜ê³  ê²°í˜¼ì„ í•˜ê³  ì‹¶ì€ ë§ˆìŒì´ ì¡°ê¸ˆì”© ìƒê¸°ëŠ” ê²ƒ ê°™ë‹¤ ê³  ê³ ë°±í–ˆë‹¤. ëŒ€ ì¤‘ë°˜ì¸ ì´ì°¬ì›ì€ ì–´ë¦° ì‹œì ˆ ë¿”í…Œ ì•ˆê²½ê³¼ í•¨ê»˜í–ˆë˜ ê³¼ê±°ë¥¼ ë– ì˜¬ë¦¬ë©° ì‹œë ¥ì´ ë§ˆì´ë„ˆìŠ¤ì˜€ëŠ”ë° ì‚´ì´ ë˜ìë§ˆì ë Œì¦ˆë¥¼ í–ˆë‹¤ ë©° ì—°ì• ë¥¼ í•˜ê³  ì‹¶ì–´ì„œ ì•ˆê²½ì„ ë²—ì—ˆë‹¤ ê³  ë§í•´ í’‹í’‹í•¨ì„ ìì•„ëƒˆë‹¤. . ì‚¬ì§„ ê°€ìˆ˜ ì¡° ë””í”¼ ì½”ë¡œë‚˜ ë¡œ ì‚¬ë§ ì—°ì˜ˆê³„ë„ íŒ¨ë‹‰ ì†¡í˜œêµ ì†”ì§ ê³ ë°± ì–´ë¦´ ë•Œë¶€í„° ë§Œë‚œ ì‚¬ëŒ ë‚´ ì˜†ì— ìˆì–´ ì„œì¥í›ˆ ë¬´ìŠ¨ ì¼ì´ì•¼ ëŒì—° ë°©ì†¡ í•˜ì°¨ ì„ ì–¸ ì œì‘ì§„ ë‹¹í™© ì„±í­í–‰ í˜ì˜ ê¹€ê±´ëª¨ ì–µ í”¼í•´ ì†ë°°ì†Œ ì²­êµ¬ ë°˜ê²© í•œì±„ì•„ ë‚¨í¸ ì°¨ì„¸ì°Œ ì§•ì—­í˜• êµ¬í˜• ì°¨ë²”ê·¼ ì°¨ë‘ë¦¬ ë¯¸ì•ˆ ì—‘ìŠ¤í¬ì¸ ë‰´ìŠ¤ ë¬´ë‹¨ ì „ì¬ ë° ì¬ë°°í¬ ê¸ˆì§€ . . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "t = get_text(url)\n",
    "cl = clean_text(t)\n",
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "def keyword_sentence(url):#í‚¤ì›Œë“œ ë¬¸ì¥ ì¶”ì¶œ\n",
    "    m_text = get_text(url)\n",
    "    r_text = clean_text(m_text)\n",
    "    data = r_text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=1, num_keysents=1)\n",
    "    keyword = list(keywords.keys())\n",
    "    return keyword[0], sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword, sents = keyword_sentence(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'í•˜ê³ '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(comments):\n",
    "    token_sent=[]\n",
    "    for comment in comments:\n",
    "        temp = okt.morphs(comment, stem=True)\n",
    "        token_sent.append(temp)\n",
    "        max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # ìƒìœ„ 35,000ê°œì˜ ë‹¨ì–´ë§Œ ë³´ì¡´\n",
    "    tokenizer.fit_on_texts(token_sent) \n",
    "    token_sent = tokenizer.texts_to_sequences(token_sent)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index)+1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(token_sent, maxlen=max_len)\n",
    "    model = load_model('model8n.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    #for i in range(len(X_data)):\n",
    "    #    if(predict[i] == 0):\n",
    "    #        a=0\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ì–´ì°¨í”¼ 3ê°œì›”ì€ ë¬´ìŠ¨ ì´ë³„í›„ í•œë‹¬ë„ ëª»ê°€ ì´ë‚¨ì ì €ë‚¨ì ê°ˆì•„íƒ€ë©´ì„œ ê³¨ê³ ë£¨ ë°•íˆë©´ì„œ ì‚´ì•„ë†“ê³  ì–¸ì œë¶€í„° ê·¸ë ‡ê²Œ ìˆœê²°í•˜ì…¨ë‹¤ê³  ê¼´ë‘ ë™ê±°3ê°œì›”ê°€ì§€ê³  ë”´ì§€ë¥¼ê±°ì‹¬? ì•„ ì™€ê¾¸ë•œì— ìì²´ ë¹„ì—°ì•  ì„ ì–¸í•˜ì‹  ê·¸ìª½ ì„±í–¥ì´ì‹ ê°€?'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê´€ë¦¬ìì•¼ ! ì„ì˜ì›…ì´ ì•„ê¸°ê³µë£¡ ë‘˜ë¦¬ì— ê¼´ëšœê¸° ì™•ì ë‹®ì•˜ë‹¤ê³  í•˜ë‹ˆê¹Œ ì™œ ì‚­ì œí•˜ëƒ !! ğŸ‘ŠğŸ˜ ê·¸ì •ë„ ë§ë„ ëª»í•˜ëƒ?\n"
     ]
    }
   ],
   "source": [
    "print(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model_load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "[0 0 1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_result)\n",
    "print(np.delete(model_result,1))\n",
    "model_result.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[1, 3, 4, 7, 10, 11, 12, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "number = []\n",
    "for i in range(1,len(model_result)):\n",
    "    if model_result[i] == 1:\n",
    "        count += 1\n",
    "        number.append(i)\n",
    "print(count)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(num):\n",
    "    count = 0\n",
    "    number = []\n",
    "    for i in range(1, len(num)):\n",
    "        if num[i] == 1:\n",
    "            count += 1\n",
    "            number.append(i)\n",
    "    return count, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "[1, 3, 4, 7, 10, 11, 12, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "a,b = count(model_result)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = [\"ìš”ì¦˜ ì—°ê¸° ì •ë§ ì˜ë³´ê³ ìˆì–´ìš”! í•­ìƒ ì‘ì›í•´ìš”\",\n",
    "        \"í•­ìƒ ì—´ì‹¬íˆ í•˜ëŠ” ëª¨ìŠµ ë„ˆë¬´ ë³´ê¸° ì¢‹ì•„ìš”:)\",\n",
    "        \"ìµœê·¼ ì‘í’ˆì—ì„œ ì—°ê¸° ì •ë§ ë§ì´ ëŠ˜ì—ˆë”ë¼ê³ ìš”!\",\n",
    "        \"ì—°ê¸°ê°€ ë„ˆë¬´ ì¢‹ì•„ì„œ ì‘ì›í•´ìš”~\",\n",
    "        \"ìš”ì¦˜ ë‚´ê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë°°ìš° ì¤‘ í•œ ëª…ã…ã…\",\n",
    "        \"ì–´ë–¤ ì—°ê¸°ë¥¼ í•´ë„ ë³´ëŠ” ì‚¬ëŒì˜ ì‹¬ê¸ˆì„ ìš¸ë¦¬ëŠ” ë°°ìš°\",\n",
    "        \"ìš”ì¦˜ ì–¼êµ´ ì •ë§ ì¢‹ì•„ë³´ì—¬ìš”\",\n",
    "        \"ì˜ˆì „ë¶€í„° ì§€ê¸ˆê¹Œì§€ ê³„ì†í•´ì„œ ì‘ì›í•˜ê³  ìˆì–´ìš”:)\",\n",
    "        \"ì—°ê¸°ë¡œëŠ” ì ˆëŒ€ ë­ë¼ ëª»í•˜ëŠ” ë°°ìš°\",\n",
    "        \"ì—°ê¸°ì‹ \",\n",
    "        \"ì •ë§ ì–¼êµ´ ì²œì¬..ê·¸ ìì²´\",\n",
    "        \"ìš”ì¦˜ ë„ˆë¬´ ì¢‹ì•„ìš”!\",\n",
    "        \"ì§„ì§œ ì†¡ê°•í˜¸ê¸‰ ì—°ê¸°ìë‹¤ ì—°ê¸°ë ˆì•Œ ê°œì˜í•¨.\",\n",
    "        \"ì—°ê¸° ë„ˆë¬´ ì˜í•´ã… ã… ã… ã… \",\n",
    "        \"ì´ë²ˆë…„ë„ ì—°ê¸° ëŒ€ìƒ ì£¼ì!!!!!!\",\n",
    "        \"ë°œì„±.ë°œìŒ.í‘œì •...ì°¸ ëª°ì…ë˜ëŠ” ë°°ìš°ë‹˜...ì‘ì›í•©ë‹ˆë‹¤.\",\n",
    "        \"ëŒ€ìƒ ë°›ì•„ì•¼í•´ìš”....\",\n",
    "        \"ë‚œ ì˜›ë‚ ë¶€í„°ì—°ê¸°ì˜í•˜ëŠ”ê±°ì•Œê³ ìˆì—ˆì§€.. \",\n",
    "        \"ì‚¬ë‘í•´ìš”!!!!!!!!!!!\",\n",
    "        \"ê± ì§€ë¦°ë‹¤.. ì—°ê¸° ë„ˆë¬´ ì˜í•˜ì‹œëŠ” ë“¯\",\n",
    "        \"ì§„ì§œ ì—°ê¸° ë¯¸ì³¤ì–´.......\",\n",
    "        \"ì§„ì§œ ê¹”ê²Œ ì—†ëŠ” ì—°ê¸°ë ¥ .. \",\n",
    "        \"í‘œì • ì—°ê¸°ê°€ ë„ˆë¬´ ì„¬ì„¸í•˜ì…”ì„œ ìˆ¨ ì•ˆì‰¬ê³  ë³´ê²Œ ë˜ëŠ”.. ì‘ì›í•©ë‹ˆë‹¤ ë°°ìš°ë‹˜ :>\",\n",
    "        \"ì—°ê¸° ë³¼ ë•Œë§ˆë‹¤ ë†€ë¼ìš” í¡ì…ë ¥ì´ ì§„ì§œ.. \",\n",
    "        \"ì§„ì‹¬ ì‹œìƒì‹ì—ì„œ ìƒ ë°›ì•„ì•¼í•  ì‹ ë“¤ë¦° ì—°ê¸°ì\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "singer = [\"ë¯¸ì„¸ë¨¼ì§€ í•œí†¨ë„ ì•ˆë¨¹ì€ ëª©ì†Œë¦¬ì˜ ì£¼ì¸ê³µ ì§„ì§œ.......\",\n",
    "         \"ìŒì•…ì„±ì€ ì§„ì§œ..ë§ë„ ì•ˆëœë‹¤ê³  ìƒê°í•œë‹¤.. \",\n",
    "         \"í•œêµ­ ê°€ìˆ˜ ì—­ì‚¬ìƒ ìµœê³ ì¸ ì‚¬ëŒ\",\n",
    "         \"ëª©ì†Œë¦¬ê°€ ì§„ì§œ ë³´ë¬¼ì´ì•¼\",\n",
    "         \"ëª©ì†Œë¦¬ê°€ ë³´ì„ê°™ë‹¤\",\n",
    "         \"ì™¸ë¡œì›€ê³¼ ë”°ëœ»í•¨ì„ ëª¨ë‘ ê°€ì§„ ê°€ìˆ˜ ëª©ì†Œë¦¬ë¥¼ ë“£ê³  ìˆìœ¼ë©´ ë‚´ë©´ì˜ ì™¸ë¡œì›€ì´ ëŠê»´ì§€ê³  ê·¸ê²Œ ì•ˆì“°ëŸ¬ì›Œ ë‹¤ê°€ê°€ë©´ ê·¸ê°€ ê°€ì§„ ë”°ìŠ¤í•¨ì— ì˜¤íˆë ¤ ìœ„ë¡œ ë°›ê²Œ ë˜ëŠ” ë‚˜ ìì‹ ì„ ë°œê²¬í•˜ê²Œ ë¨..\",\n",
    "         \"ê°™ì€ ì‹œëŒ€ì— íƒœì–´ë‚˜ ì´ ëª©ì†Œë¦¬ë¥¼ ë“¤ì„ ìˆ˜ ìˆë‹¤ëŠ”ê±´ ì •ë§ í•´ìš´ì´ì•¼\",\n",
    "         \"ìŒìƒ‰ë„ ë„ˆë¬´ ì¢‹ì§€ë§Œ ë§í•˜ëŠ”ê±° ë³´ë©´ ë§ë„ ë”°ëœ»í•˜ê³  ì˜ˆì˜ê²Œ í•˜ëŠ” ë“¯\",\n",
    "         \"ì–´ë–»ê²Œ ì‚¬ëŒì´ ì´ë ‡ê²Œ ëª¨ë“ ê²Œ ì™„ë²½í•˜ëƒ\",\n",
    "         \"ì´ ì‚¬ëŒì˜ ë…¸ë˜ë¥¼ ë“¤ìœ¼ë©° ë‚´ ì¸ìƒì˜ ìŠ¬í””ì„ ì¹˜ìœ ë°›ì•˜ë‹¤..\",\n",
    "         \"ë‚˜ì˜ ì‚¬ë‘í•˜ëŠ” ì•„í‹°ìŠ¤íŠ¸\",\n",
    "         \"ë‚´ ì¸ìƒ ê°€ìˆ˜\",\n",
    "         \"ì •ë§ ëª‡ ë²ˆì„ ë“¤ì–´ë„ ì†Œë¦„ë‹ëŠ” ê°€ì°½ë ¥ ìŒìƒ‰\",\n",
    "         \"ìŒìƒ‰ë„ ë­ë„ ë‹¤ ì¢‹ì§€ë§Œ ê°ì„±ì´ ë„ˆë¬´ ì¢‹ë‹¤\",\n",
    "         \"ë¨¼ ë¯¸ë˜ì— ë” ë” ë¹›ë‚˜ëŠ” ë®¤ì§€ì…˜ìœ¼ë¡œ ì¬í‰ê°€ ë  ê°€ìˆ˜\",\n",
    "         \"í•­ìƒ ì´ ëª©ì†Œë¦¬ ìœ ì§€í•´ì¤˜ ã… ã…  í•­ìƒ ì‘ì›í•´\",\n",
    "         \"ë‹¹ì‹ ì„ ë³´ê³  ëˆˆë¬¼ì´ ë‚˜ëŠ”ê±´ ë‹¹ì‹ ì„ ìœ„ë¡œí•˜ê³  ì‹¶ì€ ë‚˜ ë•Œë¬¸ì¸ì§€ ë‹¹ì‹ í•œí…Œ ìœ„ë¡œë°›ê³  ì‹¶ì€ ë‚˜ ë•Œë¬¸ì¸ì§€\",\n",
    "         \"ì •ë§ ê°•í•œ ì‚¬ëŒ ê°™ë‹¤. ë‚´ê°€ ìì‹ì„ ë‚³ëŠ”ë‹¤ë©´ ì´ë ‡ê²Œ í‚¤ìš°ê³  ì‹¶ì–´\",\n",
    "         \"ê°ì„±, ë³´ì»¬ì‹¤ë ¥, ì„¬ì„¸í•œ ì‚¼ë°•ì ëª¨ë‘ ê°–ì¶˜ ìµœê³ ì˜ê°€ìˆ˜\",\n",
    "         \"ì½˜ì„œíŠ¸ í•œ ë²ˆë§Œ ê°€ë³´ë©´ ì†Œì›ì´ ì—†ê² ë‹¤ ì§„ì§œë¡œ\",\n",
    "         \"ë…ë³´ì ì¸ ê°€ìˆ˜\",\n",
    "         \"ìŒì›ë³´ë‹¤ ë¼ì´ë¸Œê°€ í› ì–´ì–´ì–¼ì”¬ ì¢‹ì€ ê°€ìˆ˜\",\n",
    "         \"ì§„ì§œ ì‚¬ê¸°ìº..ì‚¬ëŒì´ ì–´ë–»ê²Œ ì´ë ‡ê²Œ ì™„ë²½í•˜ëƒ\",\n",
    "         \"ìƒˆì‚¼ìŠ¤ëŸ½ê²Œ ë§í•˜ì§€ë§Œ ë…¸ë˜ ì§„ì§œ ì˜í•´\",\n",
    "         \"ì–¸ì œë‚˜ ì‘ì›í•©ë‹ˆë‹¤:)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "entertain = [\"ê°•ì•½ ì¡°ì ˆì´ ëŠ¥ìˆ™í•œ ê°œê·¸ë§¨..ì§„ì§œ ì–´ë””ì„œë„ ëª»êµ¬í•¨\",\n",
    "            \"ì§„ì§œ ë–¡ìë¶€í„° ë„ë¥¸ì ã…‹ã…‹ã…‹ã…‹ë„ˆë¬´ ì›ƒê²¨\",\n",
    "            \"ì½”ë¯¸ë””ì–¸ ì¤‘ì—ì„œ íƒ‘ì´ë‹¤ ã…‹ã…‹\",\n",
    "            \"ë‚¨ë…€ í†µí‹€ì–´ì„œ ì´ëŸ°ì‚¬ëŒ ë“œë¬¼ê±°ê°™ì•„ ã…‹ã…‹\",\n",
    "            \"ì •ë§ ë°•ë¯¸ì„ ì²˜ëŸ¼ ë¡±ëŸ°í•˜ë©´ ì¢‹ê² ë‹¤\",\n",
    "            \"ì…ë‹´ì´ë‚˜ ì¬ì¹˜ê°€ ì§„ì§œ ë„ˆë¬´ ì‚¬ê¸°ìº..\",\n",
    "            \"ë§‰ í˜ë“¤ì–´ê°„ ì–µì§€ ê°œê·¸ ì—†ì´ ë§í•˜ë‚˜ í–‰ë™ í•˜ë‚˜í•˜ë‚˜ ë„ˆë¬´ ì›ƒê²¨ì„œ ì¢‹ì•„ìš” ã…‹ã…‹ã…‹ã…‹\",\n",
    "            \"ê°œì¢‹ì•„ ì§„ì§œë¡œ ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹\",\n",
    "            \"ë‚´ í˜¸ì ë©”ì´íŠ¸ë©´ ì¢‹ê² ë‹¤ ë„ˆë¬´ ì¢‹ìŒã…‹ã…‹ã…‹ã…‹\",\n",
    "            \"ê°œê·¸ê°€ ê¸°ë¶„ ë‚˜ì˜ì§€ ì•Šê³  ê¹”ë”í•¨\",\n",
    "            \"ì´ë²ˆë…„ë„ì— ëŒ€ìƒ ë°›ì!!!!!\",\n",
    "            \"ì§„ì§œ ë„ˆë¬´ ì¬ë¯¸ìˆì–´ ã…‹ã…‹ã…‹ã…‹\",\n",
    "            \"ì§„ì§œ ì¥ë‚œ ì•„ë‹ˆê²Œ ì›ƒê¸°ê³  ì„¼ìŠ¤ ì©”ê³  ì‚¬ë‘ìŠ¤ëŸ¬ìš´ë° ì˜¬í•´ ìƒ íœ©ì“¸ê³  ë‹¤ë‹ˆì\",\n",
    "            \"ì°œì°œí•˜ê³  ê°•ì œë¡œ ì›ƒê²Œ ë§Œë“œëŠ”ê²Œ ì•„ë‹ˆë¼ ì§„ì§œ ìì—°ìŠ¤ëŸ½ê³  ìœ ë¨¸ìŠ¤ëŸ¬ì›€ì— ê°íƒ„í•˜ë©° ê±´ê°•í•œ ì›ƒìŒì„ ì¬ì°½ì¡°í•´ë‚¸ã„´ ì´ ì‹œëŒ€ ìµœê³ \",\n",
    "            \"ì§„ì§œ ë§í•˜ëŠ” ê²ƒë¶€í„° í•˜ëŠ” ì•¡ì…˜ê¹Œì§€ ì „ë¶€ ë”° ë¼›ì†ë¶€í„° ê°œê·¸ë§¨ ã…‹ã…‹\",\n",
    "            \"ì„±ê²© ì§„ì§œ ì¢‹ì•„ë³´ì´ë”ë¼\",\n",
    "            \"ë§ë¹¨ë¡œ ì›ƒê¸°ëŠ” ì—°ì˜Œ ë“œë¬¸ë° ì§„ì§œ ë“œë¬¸ì¼€ì´ìŠ¤.\",\n",
    "            \"ì„¼ìŠ¤ìˆê²Œ ì›ƒê¸°ê¸° ì‰½ì§€ ì•Šì€ë° ì§„ì§œ ì„¼ìŠ¤ê°€ ì¥ë‚œ ì•„ë‹ˆê³  ì„±ê²©ë„ ì§„ì‹¤ë˜ì–´ë³´ì„\",\n",
    "            \"ë‚´ ë¡¤ëª¨ë¸ì´ë‹¤ ì§„ì§œë¡œ ã…‹ã…‹ã…‹ã…‹ã…‹\",\n",
    "            \"ë„ˆë¬´ íŠ¹ìƒ‰ìˆê³  ì˜¤ë°”í•˜ì§€ ì•ŠëŠ”ë°ë„ ì›ƒê²¨ ã…‹ã…‹ã…‹\",\n",
    "            \"ì§„ì§œ íƒ€ê³ ë‚˜ê¸°ë¥¼ ê°œê·¸ë§¨ì´ë‹¤..\",\n",
    "            \"ê°œê·¸ ì§„ì§œ ë‚´ ì·¨í–¥ì„\",\n",
    "            \"ë„ˆë¬´ ì—´ì‹¬íˆ í•˜ê³  ê¸°ë³¸ ì¸ì„±ì´ ì°©í•œë“¯\",\n",
    "            \"ë³´ëŠ” ì‚¬ëŒì´ ê¸°ë¶„ í•˜ë‚˜ë„ ë‚˜ì˜ì§€ ì•Šê³  ì„¼ìŠ¤ìˆê²Œ ì›ƒê²¨ì„œ ì¢‹ìŒ ã… ã… ã… ã… ã… ã… ã… \",\n",
    "            \"ì €ë ‡ê²Œ ì›ƒê¸°ê³  ì¬ì¹˜ìˆê³  ì„¼ìŠ¤ìˆê³  ê·€ì—¬ìš´ ì‚¬ëŒì´ ì„¸ìƒì— ë˜ ì–´ë”¨ì–´ ã…‹ã…‹\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "virtue = [\"ì§„ì§œ í´ë¼ìŠ¤ ë³´ì†Œ ì—­ì‹œ..\",\n",
    "         \"ê°“ê°“\",\n",
    "         \"ì´ê²Œ ë­ë¼ê³  ëˆˆì´ ì´‰ì´‰íˆ ì –ì–´ì˜¤ë„¤ìš”\",\n",
    "         \"ë¨¸ë¦¬ë¡œëŠ” ì´ë ‡ê²Œ í•´ì•¼í•œë‹¤, ì´ëŸ°í–‰ë™ì„ í•´ì•¼í•œë‹¤ê³  ìƒê°í•´ë„ í–‰ë™ìœ¼ë¡œ ì˜®ê¸°ê¸´ ì •ë§ í˜ë“¤í…ë°..ëŒ€ë‹¨í•˜ë‹¤\",\n",
    "         \"ì´ëŸ° ì„ í–‰ì´ ë” ë§ì•„ì§€ë©´ ì¢‹ê² ë‹¤\",\n",
    "         \"ì¸ì„±ì§±ì§±ì´ë‹¤\",\n",
    "         \"ì§„ì§œ ì°©í•˜ë‹¤...\",\n",
    "         \"ì¡´ê²½ìŠ¤ëŸ½ë‹¤\",\n",
    "         \"ì§„ì§œ ë©‹ìˆëŠ”ê±°ê°™ë‹¤ ã… ã… \",\n",
    "         \"ì‚¬ëŒì´ ì™œ ì¹­ì°¬ë°›ê³  í•˜ëŠ”ì§€ ë³´ë©´ ë‹¤ ì´ìœ ê°€ ìˆëŠ”ë° ì •ë§ ì°©í•˜ë‹¤\",\n",
    "         \"ë‹¹ì‹ ì€ ë„ë•ì±…..\",\n",
    "         \"ì°¸ ë©‹ìˆëŠ” ì‚¬ëŒì´ë„¤ìš”.\",\n",
    "         \"ì´ëŸ°ì¼ í•˜ëŠ”ê²Œ ì‰¬ìš´ ì¼ì€ ì•„ë‹í…ë° ì§„ì§œ ë©‹ìˆë‹¤\",\n",
    "         \"ì„¸ìƒì€ ì•„ì§ ì‚´ë§Œí•˜ë‹¤ëŠ”ê±¸ ëŠë‚€ë‹¤\",\n",
    "         \"ì°¸ ëŒ€ë‹¨í•˜ë„¤ìš” ë§ˆì¸ë“œê°€..ì¡´ê²½ìŠ¤ëŸ½ìŠµë‹ˆë‹¤\",\n",
    "         \"ì„¸ìƒì´ ë”ëŸ½ë‹¤ê³  ëŠë¼ê³  ì¢Œì ˆí•  ë•Œ ì¯¤ ì´ëŸ° ê¸°ì‚¬ë¥¼ ë³´ê²Œë¼ì„œ ë‹¤í–‰ì´ë‹¤\",\n",
    "         \"êµ‰ì¥íˆ êµì–‘ìˆì–´ë³´ì¸ë‹¤..\",\n",
    "         \"í¬ ë©‹ìˆì–´ë¼ ì ê²Œ ì¼í•˜ê³  ëˆ ë§ì´ ë²Œê¸¸~\",\n",
    "         \"ê°€ì • êµìœ¡ ì œëŒ€ë¡œ ë°›ì•˜ë„¤\",\n",
    "         \"ì•ìœ¼ë¡œ ì¢‹ì€ ê½ƒê¸¸ë§Œ ê±·ê¸¸...\",\n",
    "         \"ì•„ë¦„ë‹¤ì›Œì„œ ëˆˆë¬¼ì´ ë‚ ë ¤í•˜ë„¤ìš”  ì°¸ ì°©í•´ìš”\",\n",
    "         \"ì²œì‚¬ê°€ ì—¬ê¸°ìˆë„¤\",\n",
    "         \"ì´ëŸ° ì‚¬ëŒë³´ê³  ì„±ì¸ì´ë¼ê³  í•˜ëŠ”ê±°ì§€\",\n",
    "         \"ë³´ë©´ ë³¼ìˆ˜ë¡ ì •ë§ ê´œì°®ì€ ì‚¬ëŒì´ë¼ëŠ”ê²Œ ëŠê»´ì§„ë‹¤\",\n",
    "         \"ì—°ì˜ˆì¸ì´ ì•„ë‹ˆë¼ ì¸ê°„ëŒ€ ì¸ê°„ìœ¼ë¡œ ë³´ì•„ë„ ê´œì°®ì€ ì‚¬ëŒê°™ë‹¤\",\n",
    "         \"í¬ í´ë¼ìŠ¤ ë³´ì†Œ\",\n",
    "         \"ì°¸ ì¢‹ì€ ì‚¬ëŒì¸ ê²ƒ ê°™ì•„\",\n",
    "         \"ì‚¬ëŒì„ í–‰ë³µí•˜ê²Œ í•˜ëŠ” ë¹„íƒ€ë¯¼ ê°™ì€ ì¡´ì¬..í•­ìƒ ì¢‹ì€ ì¼ë§Œ ìˆìœ¼ë©´ ì¢‹ê² ë‹¤\",\n",
    "         \"ì¸ì„± ì§„ì ìµœê³ ì¸ê±°ê°™ë‹¤\",\n",
    "         \"ì¸ì„±í‚¹\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_c = [\"ì—°ê¸°\", \"ë°°ìš°\", \"ì‘í’ˆ\", \"ì°¨ê¸°ì‘\", \"ì˜í™”\", \"ë“œë¼ë§ˆ\", \"ì‹œì²­\"]\n",
    "singer_c = [\"ìŒìƒ‰\", \"ì½˜ì„œíŠ¸\", \"ë¬´ëŒ€\", \"ë…¸ë˜\", \"ê°€ìˆ˜\", \"ê·¸ë£¹\", \"ì•„ì´ëŒ\", \"ê±¸ê·¸ë£¹\", \"ì¶¤\", \"ëŒ„ìŠ¤\", \"ì‘ì‚¬\", \"ì‘ê³¡\"]\n",
    "entertain_c = [\"ì˜ˆëŠ¥\", \"ê°œê·¸ë§¨\", \"ê°œê·¸ìš°ë¨¼\",\"ìœ ìŠ¹í˜¸\"]\n",
    "virtue_c = [\"ê¸°ë¶€\", \"ì„ í–‰\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "result = []\n",
    "\n",
    "def select(text, number):#ê¸°ì‚¬ ë‚´ìš© ì¤‘ ë‹¨ì–´ ì„ ë³„í•˜ê¸°\n",
    "    lines = []\n",
    "    line2 = []\n",
    "    for word in text:\n",
    "        lines = str.split(text,\".\")\n",
    "    for each_line in lines:\n",
    "        if each_line.find(\"ê¸°ë¶€\") > 0:\n",
    "            result = random.sample(virtue, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì„ í–‰\") > 0:\n",
    "            result = random.sample(virtue, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì—°ê¸°\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break\n",
    "        elif each_line.find(\"ë°°ìš°\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break \n",
    "        elif each_line.find(\"ì‘í’ˆ\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì°¨ê¸°ì‘\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì˜í™”\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break\n",
    "        elif each_line.find(\"ë“œë¼ë§ˆ\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì‹œì²­\") > 0:\n",
    "            result = random.sample(actor, number)\n",
    "            break\n",
    "        elif each_line.find(\"ìŒìƒ‰\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì½˜ì„œíŠ¸\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ë¬´ëŒ€\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ë…¸ë˜\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ê°€ìˆ˜\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ê·¸ë£¹\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì•„ì´ëŒ\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ê±¸ê·¸ë£¹\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì¶¤\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ëŒ„ìŠ¤\") > 0:\n",
    "            result = random.sample(singer, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì˜ˆëŠ¥\") > 0:\n",
    "            result = random.sample(entertain, number)\n",
    "            break\n",
    "        elif each_line.find(\"ê°œê·¸ë§¨\") > 0:\n",
    "            result = random.sample(entertain, number)\n",
    "            break\n",
    "        elif each_line.find(\"ê°œê·¸ìš°ë¨¼\") > 0:\n",
    "            result = random.sample(entertain, number)\n",
    "            break\n",
    "        elif each_line.find(\"ì˜ˆëŠ¥\") > 0:\n",
    "            result = random.sample(entertain, number)\n",
    "            break\n",
    "        else:\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ë‹¹ì‹ ì„ ë³´ê³  ëˆˆë¬¼ì´ ë‚˜ëŠ”ê±´ ë‹¹ì‹ ì„ ìœ„ë¡œí•˜ê³  ì‹¶ì€ ë‚˜ ë•Œë¬¸ì¸ì§€ ë‹¹ì‹ í•œí…Œ ìœ„ë¡œë°›ê³  ì‹¶ì€ ë‚˜ ë•Œë¬¸ì¸ì§€'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = select(cl, a)\n",
    "text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ë‹¹ì‹ ì„ ë³´ê³  ëˆˆë¬¼ì´ ë‚˜ëŠ”ê±´ ë‹¹ì‹ ì„ ìœ„ë¡œí•˜ê³  ì‹¶ì€ ë‚˜ ë•Œë¬¸ì¸ì§€ ë‹¹ì‹ í•œí…Œ ìœ„ë¡œë°›ê³  ì‹¶ì€ ë‚˜ ë•Œë¬¸ì¸ì§€',\n",
       " 'ê°™ì€ ì‹œëŒ€ì— íƒœì–´ë‚˜ ì´ ëª©ì†Œë¦¬ë¥¼ ë“¤ì„ ìˆ˜ ìˆë‹¤ëŠ”ê±´ ì •ë§ í•´ìš´ì´ì•¼',\n",
       " 'ì™¸ë¡œì›€ê³¼ ë”°ëœ»í•¨ì„ ëª¨ë‘ ê°€ì§„ ê°€ìˆ˜ ëª©ì†Œë¦¬ë¥¼ ë“£ê³  ìˆìœ¼ë©´ ë‚´ë©´ì˜ ì™¸ë¡œì›€ì´ ëŠê»´ì§€ê³  ê·¸ê²Œ ì•ˆì“°ëŸ¬ì›Œ ë‹¤ê°€ê°€ë©´ ê·¸ê°€ ê°€ì§„ ë”°ìŠ¤í•¨ì— ì˜¤íˆë ¤ ìœ„ë¡œ ë°›ê²Œ ë˜ëŠ” ë‚˜ ìì‹ ì„ ë°œê²¬í•˜ê²Œ ë¨..',\n",
       " 'ì´ ì‚¬ëŒì˜ ë…¸ë˜ë¥¼ ë“¤ìœ¼ë©° ë‚´ ì¸ìƒì˜ ìŠ¬í””ì„ ì¹˜ìœ ë°›ì•˜ë‹¤..',\n",
       " 'ì½˜ì„œíŠ¸ í•œ ë²ˆë§Œ ê°€ë³´ë©´ ì†Œì›ì´ ì—†ê² ë‹¤ ì§„ì§œë¡œ',\n",
       " 'ìŒìƒ‰ë„ ë­ë„ ë‹¤ ì¢‹ì§€ë§Œ ê°ì„±ì´ ë„ˆë¬´ ì¢‹ë‹¤',\n",
       " 'ë…ë³´ì ì¸ ê°€ìˆ˜',\n",
       " 'ìƒˆì‚¼ìŠ¤ëŸ½ê²Œ ë§í•˜ì§€ë§Œ ë…¸ë˜ ì§„ì§œ ì˜í•´',\n",
       " 'ìŒìƒ‰ë„ ë„ˆë¬´ ì¢‹ì§€ë§Œ ë§í•˜ëŠ”ê±° ë³´ë©´ ë§ë„ ë”°ëœ»í•˜ê³  ì˜ˆì˜ê²Œ í•˜ëŠ” ë“¯',\n",
       " 'ë‚´ ì¸ìƒ ê°€ìˆ˜',\n",
       " 'ê°ì„±, ë³´ì»¬ì‹¤ë ¥, ì„¬ì„¸í•œ ì‚¼ë°•ì ëª¨ë‘ ê°–ì¶˜ ìµœê³ ì˜ê°€ìˆ˜']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete(result, number):\n",
    "    new = np.delete(result, number)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_comment(comments, number):\n",
    "    new = np.delete(comments, number)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stra = de_comment(s, b)\n",
    "stra_num = delete(model_result, b)\n",
    "print(stra)\n",
    "print(stra_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count2(num): #countëŠ” ì•…í”Œ ê°œìˆ˜, numberì€ ì•…í”Œ ìœ„ì¹˜\n",
    "    number = []\n",
    "    for i in range(1, len(num)):\n",
    "        if num[i] == 0:\n",
    "            number.append(i)\n",
    "    return number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def de_comment2(comments, number):\n",
    "    new = np.delete(comments, number)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = count2(model_result)\n",
    "new = de_comment(s, c)\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comment_data = pd.DataFrame(s)\n",
    "comment_data.columns = ['comment']\n",
    "comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_result = np.insert(model_result, 0, 0)\n",
    "de_result = np.delete(in_result,0)\n",
    "print(de_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_data = pd.DataFrame(de_result)\n",
    "de_data.columns = ['label']\n",
    "de_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.concat([de_data, comment_data], axis = 1)\n",
    "df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = df_new['comment']\n",
    "y_data = df_new['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_text = []\n",
    "for string in X_data.tolist():\n",
    "    try:\n",
    "        tokens = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£]+\", \" \", string.lower())\n",
    "    except Exception as e:\n",
    "        print(string)\n",
    "        break\n",
    "    normalized_text.append(tokens)\n",
    "print(normalized_text[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_new.comment = normalized_text\n",
    "for sentence in df_new['comment']:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "def train(comments, model_result):\n",
    "    comment_data = pd.DataFrame(comments)\n",
    "    comment_data.columns = ['comment']\n",
    "    in_result = np.insert(model_result, 0, 0)\n",
    "    de_result = np.delete(in_result,0)\n",
    "    de_data = pd.DataFrame(de_result)\n",
    "    de_data.columns = ['label']\n",
    "    df_new = pd.concat([de_data, comment_data], axis = 1)\n",
    "    X_data = df_new['comment']\n",
    "    y_data = df_new['label']\n",
    "    normalized_text = []\n",
    "    for string in X_data.tolist():\n",
    "        try:\n",
    "            tokens = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£]+\", \" \", string.lower())\n",
    "        except Exception as e:\n",
    "            break\n",
    "    normalized_text.append(tokens)\n",
    "    #df_new.comment = normalized_text\n",
    "    okt = Okt()\n",
    "    X_token=[]\n",
    "    for sentence in df_new['comment']:\n",
    "        temp_X = []\n",
    "        temp_X = okt.morphs(sentence, stem=True) # í† í°í™”\n",
    "        X_token.append(temp_X)\n",
    "    max_words = 10000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # ìƒìœ„ 35,000ê°œì˜ ë‹¨ì–´ë§Œ ë³´ì¡´\n",
    "    tokenizer.fit_on_texts(X_token) \n",
    "    X_token = tokenizer.texts_to_sequences(X_token)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index) + 1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(X_token, maxlen=max_len)\n",
    "    y_data = np.array(y_data).reshape(-1, 1)\n",
    "    model = load_model('model8n.h5')\n",
    "    history = model.fit(X_data, y_data, epochs=5, batch_size=32)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(s, model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articeBody'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ã„±-ã…ã…-ã…£ê°€-í£\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = soup.select_one('h2.end_tit')\n",
    "    text_r = text.get_text()\n",
    "    return text_r\n",
    "title = get_title('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "def keyword_sentence(url):#í‚¤ì›Œë“œ ë¬¸ì¥ ì¶”ì¶œ\n",
    "    m_text = get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "    r_text = clean_text(m_text)\n",
    "    data = r_text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=11, num_keysents=10)\n",
    "    keyword = list(keywords.keys())\n",
    "    return keyword[0], sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword, sents = keyword_sentence(clean)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
    "    global keyword\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    init_word = current_word \n",
    "    sentence = ''\n",
    "    for _ in range(n): # në²ˆ ë°˜ë³µ\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # í˜„ì¬ ë‹¨ì–´ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # ë°ì´í„°ì— ëŒ€í•œ íŒ¨ë”©\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ yë¥¼ ì˜ˆì¸¡í•˜ê³  y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
    "        for k, index in t.word_index.items(): \n",
    "            if index == result: # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´\n",
    "                break # í•´ë‹¹ ë‹¨ì–´ê°€ ì˜ˆì¸¡ ë‹¨ì–´ì´ë¯€ë¡œ break\n",
    "        current_word = current_word + ' '  + k # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
    "        sentence = sentence + ' ' + k # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
    "    # forë¬¸ì´ë¯€ë¡œ ì´ í–‰ë™ì„ ë‹¤ì‹œ ë°˜ë³µ\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def clean_sentence(word):\n",
    "    global keyword\n",
    "    max_words = 35000\n",
    "    clean_sen = []\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # ëª¨ë¸, í† í¬ë‚˜ì´ì €, í˜„ì¬ ë‹¨ì–´, ë°˜ë³µí•  íšŸìˆ˜\n",
    "    global word\n",
    "    init_word = current_word # ì²˜ìŒ ë“¤ì–´ì˜¨ ë‹¨ì–´ë„ ë§ˆì§€ë§‰ì— ê°™ì´ ì¶œë ¥í•˜ê¸°ìœ„í•´ ì €ì¥\n",
    "    sentence = ''\n",
    "    for _ in range(n): # në²ˆ ë°˜ë³µ\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # í˜„ì¬ ë‹¨ì–´ì— ëŒ€í•œ ì •ìˆ˜ ì¸ì½”ë”©\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # ë°ì´í„°ì— ëŒ€í•œ íŒ¨ë”©\n",
    "        result = model.predict_classes(encoded, verbose=0)# ì…ë ¥í•œ X(í˜„ì¬ ë‹¨ì–´)ì— ëŒ€í•´ì„œ yë¥¼ ì˜ˆì¸¡í•˜ê³  y(ì˜ˆì¸¡í•œ ë‹¨ì–´)ë¥¼ resultì— ì €ì¥.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # ë§Œì•½ ì˜ˆì¸¡í•œ ë‹¨ì–´ì™€ ì¸ë±ìŠ¤ì™€ ë™ì¼í•œ ë‹¨ì–´ê°€ ìˆë‹¤ë©´\n",
    "                break # í•´ë‹¹ ë‹¨ì–´ê°€ ì˜ˆì¸¡ ë‹¨ì–´ì´ë¯€ë¡œ break\n",
    "        current_word = current_word + ' '  + word # í˜„ì¬ ë‹¨ì–´ + ' ' + ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ í˜„ì¬ ë‹¨ì–´ë¡œ ë³€ê²½\n",
    "        sentence = sentence + ' ' + word # ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ë¬¸ì¥ì— ì €ì¥\n",
    "    # forë¬¸ì´ë¯€ë¡œ ì´ í–‰ë™ì„ ë‹¤ì‹œ ë°˜ë³µ\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(word):\n",
    "    clean_sen = []\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(model, t, word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
