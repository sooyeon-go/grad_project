{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://news.nate.com/view/20200314n09669?mid=n1008'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='realArtcContents'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\.]+\", \" \", str(text))\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'html.parser', from_encoding='utf-8')\n",
    "    text = soup.find('h3',class_='articleSubecjt')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wd = \"C:\\\\Users\\\\user\\\\Downloads\\\\chromedriver_win32\\\\chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment(url):\n",
    "    driver = webdriver.Chrome(wd)\n",
    "    url_nums = re.findall(\"\\d+\", url)\n",
    "    addr = 'http://comm.news.nate.com/Comment/ArticleComment/List?artc_sq='+url_nums[0]+'n'+url_nums[1] # 댓글창 주소\n",
    "    driver.get(addr)\n",
    "    pages = 2\n",
    "    try:\n",
    "        for i in range(5):\n",
    "            WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.CSS_SELECTOR,\"body > div.commentBox.reply_hide > div:nth-child(3) > div.paging_wrap > a:nth-child(\"+str(pages)+\")\"))).click()\n",
    "            time.sleep(2)\n",
    "            print(pages, end=\" \")\n",
    "            pages+=1\n",
    "    except exceptions.ElementNotVisibleException as e: # 페이지 끝\n",
    "        pass\n",
    "    except Exception as e: # 다른 예외 발생시 확인\n",
    "        print(e)\n",
    "    html = driver.page_source\n",
    "    dom = BeautifulSoup(html, \"lxml\")\n",
    "    # 댓글이 들어있는 페이지 전체 크롤링\n",
    "    comments_raw = dom.find_all('dd',{'class':'usertxt'})\n",
    "    # 댓글의 text만 뽑는다.\n",
    "    comments = [comment.text.strip() for comment in comments_raw]\n",
    "    return comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 3 4 5 6 "
     ]
    },
    {
     "data": {
      "text/plain": [
       "['집으로 꼬찔찔이 연기잘하던 꼬맹이가 \\n이렇게 멋진 남자 배우가 되었는데\\n아역배우중 가장 역변없이 잘생긴 배우로 성장한 \\n성공한 케이스중 하나일듯',\n",
       " '근데 내가 유승호 얼굴이었으면 얼굴 지적당해도 매일 행복할거같은데.. 그렇다고..',\n",
       " '유승호 외모 지적ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ\\n유승호가 80kg까지 나가도 강남 일대 씹어먹을텐데 ㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋㅋ',\n",
       " '내가 저얼굴이면 누가 외모지적해도 피식도 안할거같은데 맘이 여린가',\n",
       " '솔직히 급 노화...\\n아저씨 다됐음ㅋ',\n",
       " '너무 예민한것 아닌지.',\n",
       " '유승호를 외모로 까?ㅋㅋㅋ',\n",
       " '승호야ㅋㅋㅋㅋㅋㅋ 한귀로 듣고 흘릴 가치도 없다 걍 무시해ㅋㅋㅋ',\n",
       " '살찐것도 모르겠고 잘생기기만 했어',\n",
       " '어디가 살쪘다는건지 알수가 없네요? 훈남인데 별 악플이다있네요 드라마 잼있구 본방사수중이에요~^^',\n",
       " '유승호가 100kg여도 나보다는 잘생겼을거같은데;',\n",
       " '근데 유승호도 연기가 참안늘어  연기에 깊이가 없다고 할까?',\n",
       " '어디가 쪘다는 건지? 잘생겼는뎅~~',\n",
       " '아니 누가 누굴보고 외모 지적을해?\\n남자가 봐도 존잘인데 뭐지;;',\n",
       " '외모고 자시고 연기도 별로고 드라마가 너무 재미없더라ㅋㅋㅋ',\n",
       " '오징어들이 누구를 건드나',\n",
       " '아니 유승호를 외모로 깐다고 ????',\n",
       " '우리나라에서 제일 잘생긴 배우 중 하나라 생각했는데 유승호도 외모악플이 신경쓰이는게 신기하다... 연기잔데 배역따라서 살 좀 찌울 수 있지',\n",
       " '악플러들 불러다가 같이 셀카찍어 인스타 올려라 승호야 ㅋㅋㅋ',\n",
       " '잘생겼구만  지적한사람면상보자']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = comment(url)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 톱스타뉴스 김효진 메모리스트 유승호가 외모 악플에 대한 심경을 고백한 후 인스타그램을 업데이트했다. 앞서 지난 일 유승호는 자신의 인스타스토리를 통해 경찰 역할이라 일부러 살 많이 찌웠어요. 저도 알아요 얼굴 살찐 거 라는 글을 남긴 바 있다. 일부 네티즌의 악플에 대다수의 네티즌은 잘생겼는데 어디가 찐거냐 며 이해가 안된다는 반응을 보였다. 자신의 글이 이슈가 되자 유승호는 결국 인스타스토리를 삭제했고 메모리스트 촬영 사진을 남기며 인스타그램 활동을 이어가고 있다. 일 오후 유승호는 자신의 인스타그램에 웃자 라는 글과 함께 사진 한 장을 게재했다. 공개된 사진 속에는 경찰 제복을 입고 메모리스트 를 촬영 중인 유승호의 모습이 담겨 있다. 특히 유승호의 환한 미소가 훈훈함을 자아낸다. 극중 유승호는 사이코메트리 초능력을 가진 형사이자 전 세계 유일무이 공인된 초능력자 동백 역을 맡아 열연 중에 있다. 한편 유승호 이세영 등이 출연 중인 메모리스트 는 매주 수 목요일 오후 시 분 방송된다. 김효진 . . 취재 및 보도 . . 톱스타뉴스 . . 무단전재 재배포 금지 대한민국 . 스타 사진뉴스 . 사진 을 클릭하면 국내 최대 사이즈인 사진을 볼 수 있습니다. 메모리스트 유승호 붉은 돼지 발견했다 재방송 넷플릭스 방영 여부는 메모리스트 유승호 검찰 허수아비 때려눕힌 맨손 액션 통쾌 직캠 유승호 년 올해는 유승호가 대세 유승호 외모 악플에 심경 고백 인스타 글 삭제 네티즌 무분별 비난 그만 지적 메모리스트 유승호 이세영 회 예고 공개 뜨거운 시청자 반응 . . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "t = get_text(url)\n",
    "cl = clean_text(t)\n",
    "print(cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "def keyword_sentence(url):#키워드 문장 추출\n",
    "    m_text = get_text(url)\n",
    "    r_text = clean_text(m_text)\n",
    "    data = r_text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=11, num_keysents=10)\n",
    "    keyword = list(keywords.keys())\n",
    "    return keyword[0], sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 자신의 글이 이슈가 되자 유승호는 결국 인스타스토리를 삭제했고 메모리스트 촬영 사진을 남기며 인스타그램 활동을 이어가고 있다',\n",
       " ' 앞서 지난 일 유승호는 자신의 인스타스토리를 통해 경찰 역할이라 일부러 살 많이 찌웠어요',\n",
       " ' 스타 사진뉴스 ',\n",
       " ' 저도 알아요 얼굴 살찐 거 라는 글을 남긴 바 있다',\n",
       " ' 일부 네티즌의 악플에 대다수의 네티즌은 잘생겼는데 어디가 찐거냐 며 이해가 안된다는 반응을 보였다',\n",
       " ' 김효진 ',\n",
       " ' ',\n",
       " ' 취재 및 보도 ',\n",
       " ' ',\n",
       " ' 톱스타뉴스 ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword, sents = keyword_sentence(url)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'유승호'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from konlpy.tag import Okt  \n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.common import exceptions\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_load(comments):\n",
    "    token_sent=[]\n",
    "    for comment in comments:\n",
    "        temp = okt.morphs(comment, stem=True)\n",
    "        token_sent.append(temp)\n",
    "        max_words = 35000\n",
    "    tokenizer = Tokenizer(num_words = max_words) # 상위 35,000개의 단어만 보존\n",
    "    tokenizer.fit_on_texts(token_sent) \n",
    "    token_sent = tokenizer.texts_to_sequences(token_sent)\n",
    "    word_to_index = tokenizer.word_index\n",
    "    vocab_size = len(word_to_index)+1\n",
    "    max_len = 124\n",
    "    X_data = pad_sequences(token_sent, maxlen=max_len)\n",
    "    model = load_model('model8n.h5')\n",
    "    predict = model.predict_classes(X_data)\n",
    "    #for i in range(len(X_data)):\n",
    "    #    if(predict[i] == 0):\n",
    "    #        a=0\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'집으로 꼬찔찔이 연기잘하던 꼬맹이가 \\n이렇게 멋진 남자 배우가 되었는데\\n아역배우중 가장 역변없이 잘생긴 배우로 성장한 \\n성공한 케이스중 하나일듯'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'근데 내가 유승호 얼굴이었으면 얼굴 지적당해도 매일 행복할거같은데.. 그렇다고..'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model_load(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dtype('int32')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model_result)\n",
    "#print(np.delete(model_result,1))\n",
    "model_result.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[6, 7, 8, 10, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "number = []\n",
    "for i in range(1,len(model_result)):\n",
    "    if model_result[i] == 1:\n",
    "        count += 1\n",
    "        number.append(i)\n",
    "print(count)\n",
    "print(number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(num):\n",
    "    count = 0\n",
    "    number = []\n",
    "    for i in range(1, len(num)):\n",
    "        if num[i] == 1:\n",
    "            count += 1\n",
    "            number.append(i)\n",
    "    return count, number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "[6, 7, 8, 10, 14, 16]\n"
     ]
    }
   ],
   "source": [
    "a,b = count(model_result)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = ''\n",
    "    for item in soup.find_all('div', id='articeBody'):\n",
    "        text = text + str(item.find_all(text=True))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt=get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r\"[^ㄱ-ㅎㅏ-ㅣ가-힣\\.]+\", \" \", text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = clean_text(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\t\\t\\t'놀면 뭐하니' 유재석, 절친♥︎들과 남산→서점→방 탈출..포상휴가 '만끽'[종합]\\n\\t\\t\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_title(url):\n",
    "    source_code_from_URL = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(source_code_from_URL, 'lxml', from_encoding='utf-8')\n",
    "    text = soup.select_one('h2.end_tit')\n",
    "    text_r = text.get_text()\n",
    "    return text_r\n",
    "title = get_title('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 놀면 뭐하니 유재석 절친 들과 남산 서점 방 탈출..포상휴가 만끽 종합 '"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from krwordrank.sentence import summarize_with_sentences\n",
    "def keyword_sentence(url):#키워드 문장 추출\n",
    "    m_text = get_text('https://entertain.naver.com/read?oid=109&aid=0004166734')\n",
    "    r_text = clean_text(m_text)\n",
    "    data = r_text.split('.')\n",
    "    keywords, sents = summarize_with_sentences(data, num_keywords=11, num_keysents=10)\n",
    "    keyword = list(keywords.keys())\n",
    "    return keyword[0], sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' 놀면 뭐하니 방송화면 캡처 김보라 기자 유재석 지석진 이광수 조세호가 포상 휴가를 즐겼다',\n",
       " ' 조세호도 전문적으로 웃음을 드린다 고 소개했는데 유재석이 전문적인 거 치곤 조금 아쉽다 고 하자 조세호는 근근이 밥 벌어 먹고 산다 고 받아쳤다',\n",
       " ' 이광수의 의견에 따라 유재석은 남산 돈가스 집까지 걸어 가자고 했다',\n",
       " ' 이에 조세호는 우리가 만만한 거다 라고 맞아쳐 웃음을 안겼다',\n",
       " ' 조세호와 이광수도 평상시에 오는 건 좋은데 휴가에 여길 오냐 고 했다',\n",
       " ' 아내와 아이와 행복을 위해 달려나가고 있다 고 했고 이광수는 저는 개그맨은 아니지만 주말 예능을 하고 있다 고 말해 웃음을 안겼다',\n",
       " ' 세 사람은 유재석과 단둘이 대 로 떠나는 여행인 줄 알고 왔는데 알고 보니 인이 떠나는 여행이 됐다',\n",
       " ' 우리 넷이 어디 가는 거 처음이다 라고 기뻐하는 유재석',\n",
       " ' 지석진은 이게 이 프로그램의 위엄이다',\n",
       " ' 걷는 걸 좋아한다 며 여행하면서 차 타는 것도 좋지만 걸으며 보는 걸 좋아한다 고 말했다']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword, sents = keyword_sentence(clean)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'캡처'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    global keyword\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    init_word = current_word \n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)\n",
    "    # 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for k, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + k # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + k # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def clean_sentence(word):\n",
    "    global keyword\n",
    "    max_words = 35000\n",
    "    clean_sen = []\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'k' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-8a187819ae4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mclean_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-48-ba012d1d4d07>\u001b[0m in \u001b[0;36mclean_sentence\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mclean_sen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_generation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mclean_sen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-47-00ef959e2e49>\u001b[0m in \u001b[0;36msentence_generation\u001b[1;34m(current_word, n)\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m                 \u001b[1;32mbreak\u001b[0m \u001b[1;31m# 해당 단어가 예측 단어이므로 break\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mcurrent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_word\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;31m# 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mk\u001b[0m \u001b[1;31m# 예측 단어를 문장에 저장\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# for문이므로 이 행동을 다시 반복\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'k' referenced before assignment"
     ]
    }
   ],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(model, t, current_word, n): # 모델, 토크나이저, 현재 단어, 반복할 횟수\n",
    "    global word\n",
    "    init_word = current_word # 처음 들어온 단어도 마지막에 같이 출력하기위해 저장\n",
    "    sentence = ''\n",
    "    for _ in range(n): # n번 반복\n",
    "        encoded = t.texts_to_sequences([current_word])[0] # 현재 단어에 대한 정수 인코딩\n",
    "        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # 데이터에 대한 패딩\n",
    "        result = model.predict_classes(encoded, verbose=0)# 입력한 X(현재 단어)에 대해서 y를 예측하고 y(예측한 단어)를 result에 저장.\n",
    "        for word, index in t.word_index.items(): \n",
    "            if index == result: # 만약 예측한 단어와 인덱스와 동일한 단어가 있다면\n",
    "                break # 해당 단어가 예측 단어이므로 break\n",
    "        current_word = current_word + ' '  + word # 현재 단어 + ' ' + 예측 단어를 현재 단어로 변경\n",
    "        sentence = sentence + ' ' + word # 예측 단어를 문장에 저장\n",
    "    # for문이므로 이 행동을 다시 반복\n",
    "    sentence = init_word + sentence\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(word):\n",
    "    clean_sen = []\n",
    "    model = load_model('model_create.h5')\n",
    "    max_words = 35000\n",
    "    t = Tokenizer(num_words = max_words)\n",
    "    for i in range(0,10):\n",
    "        sentence = sentence_generation(model, t, word, i)\n",
    "        clean_sen.append(sentence)\n",
    "        i += 1\n",
    "    return clean_sen[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence(keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
